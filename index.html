<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Nicolas Audebert - Computer Vision & Deep Learning Researcher</title>
        <meta charset="UTF-8" />
        <meta name="referrer" content="none" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
	<meta http-equiv="Content-Security-Policy" content="script-src 'self'">
        <link href="static/css/audebert.at.css" rel="stylesheet">
    </head>

    <body>
        <header>
        </header>

        <nav>
            <h3 class="navlink" id="sitename"><a href="">Nicolas Audebert</a></h3>
            <h3 class="navlink"><a href="#news">News</a></h3>
            <h3 class="navlink"><a href="#research">Research</a></h3>
            <h3 class="navlink"><a href="#code">Code/datasets</a></h3>
            <h3 class="navlink"><a href="#publications">Publications</a></h3>
            <h3 class="navlink"><a href="#teaching">Teaching</a></h3>
            <h3 class="navlink"><a href="#resume">Resume</a></h3>
        </nav>
        <div class="flex-container">

        <aside>
            <div class="picture"><img src="static/img/na2.jpg" width=150 height=150 alt="Profile picture" /></div>
            <h1>Nicolas Audebert</h1>
            <section id="about">
                <h2>Computer Vision and Machine Learning researcher</h2>
                <h2>Assistant professor of Computer Science (maître de conférences)</h2>
                <h2>Conservatoire National des Arts et Métiers (CNAM)</h2>
                <ul>
                    <li><a href="https://github.com/nshaud">
                        <svg class="icon" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub icon</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg> 
                        <span>GitHub</span></a></li>
                    <li><a href="https://twitter.com/nshaud"><svg class="icon" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Twitter icon</title><path d="M23.954 4.569c-.885.389-1.83.654-2.825.775 1.014-.611 1.794-1.574 2.163-2.723-.951.555-2.005.959-3.127 1.184-.896-.959-2.173-1.559-3.591-1.559-2.717 0-4.92 2.203-4.92 4.917 0 .39.045.765.127 1.124C7.691 8.094 4.066 6.13 1.64 3.161c-.427.722-.666 1.561-.666 2.475 0 1.71.87 3.213 2.188 4.096-.807-.026-1.566-.248-2.228-.616v.061c0 2.385 1.693 4.374 3.946 4.827-.413.111-.849.171-1.296.171-.314 0-.615-.03-.916-.086.631 1.953 2.445 3.377 4.604 3.417-1.68 1.319-3.809 2.105-6.102 2.105-.39 0-.779-.023-1.17-.067 2.189 1.394 4.768 2.209 7.557 2.209 9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63.961-.689 1.8-1.56 2.46-2.548l-.047-.02z"/></svg> 
                        <span>Twitter</span></a></li>
                    <li><a href="https://orcid.org/0000-0001-6486-3102">
                        <svg class="icon" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>ORCID icon</title><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"/></svg> 
                        <span>ORCID</span></a></li>
                    <li><a href="https://scholar.google.com/citations?user=_z5vXUcAAAAJ">
                        <svg class="icon" role="img" viewBox="0 0 255 255" xmlns="http://www.w3.org/2000/svg"><title>Google Scholar icon</title><path d="M127.5,0C57.196,0,0,57.196,0,127.5S57.196,255,127.5,255S255,197.804,255,127.5S197.804,0,127.5,0z M127.5,240 C65.467,240,15,189.533,15,127.5S65.467,15,127.5,15S240,65.467,240,127.5S189.533,240,127.5,240z M127.5,104.271 c-11.696,0-21.211,9.516-21.211,21.211s9.516,21.211,21.211,21.211c11.696,0,21.211-9.516,21.211-21.211 S139.196,104.271,127.5,104.271z M127.5,131.694c-3.425,0-6.211-2.787-6.211-6.211s2.787-6.211,6.211-6.211 c3.425,0,6.211,2.787,6.211,6.211S130.925,131.694,127.5,131.694z M194.351,92.261c0.567-1.933,1.101-3.849,1.561-5.728 c5.546-22.602,2.571-39.18-8.378-46.68c-3.87-2.651-8.471-3.995-13.674-3.995c-12.948,0-29.48,8.806-46.361,23.809 c-16.881-15.003-33.413-23.809-46.361-23.809c-5.203,0-9.804,1.344-13.674,3.995c-10.949,7.5-13.924,24.078-8.377,46.68 c0.461,1.879,0.994,3.795,1.562,5.729c-1.915,0.624-3.797,1.267-5.607,1.948c-21.782,8.196-33.778,20.019-33.778,33.29 c0,13.272,11.996,25.095,33.778,33.292c1.811,0.681,3.692,1.325,5.607,1.948c-0.567,1.933-1.101,3.849-1.561,5.728 c-5.547,22.602-2.572,39.18,8.378,46.681c3.87,2.65,8.47,3.994,13.673,3.994c0.001,0,0,0,0.001,0c12.947,0,29.48-8.805,46.36-23.808 c16.881,15.003,33.413,23.808,46.361,23.808c5.203,0,9.803-1.344,13.674-3.995c10.949-7.501,13.924-24.079,8.377-46.68 c-0.461-1.879-0.994-3.795-1.561-5.728c1.914-0.624,3.796-1.267,5.607-1.948c21.782-8.197,33.778-20.02,33.778-33.292 s-11.996-25.095-33.778-33.291C198.147,93.527,196.266,92.884,194.351,92.261z M173.861,50.857c2.151,0,3.851,0.448,5.197,1.37 c5.607,3.841,6.505,17.156,0.904,36.088c-9.15-2.072-19.068-3.572-29.439-4.474c-4.004-4.879-8.074-9.46-12.165-13.711 C152.839,57.465,165.755,50.857,173.861,50.857z M164.341,152.733c2.215-3.234,4.332-6.489,6.368-9.753 c1.556,3.266,2.96,6.441,4.223,9.522c-3.762,0.809-7.742,1.53-11.929,2.152C163.448,154.015,163.897,153.38,164.341,152.733z  M163.038,100.398c4.086,0.613,8.047,1.337,11.867,2.162c-1.256,3.061-2.651,6.216-4.197,9.459c-2.035-3.264-4.153-6.52-6.368-9.753 C163.909,101.637,163.472,101.021,163.038,100.398z M162.543,127.5c-3.141,5.434-6.656,11.032-10.577,16.757 c-3.004,4.386-6.048,8.531-9.105,12.457c-4.92,0.287-10.044,0.44-15.362,0.44c-5.317,0-10.44-0.153-15.361-0.44 c-3.056-3.926-6.101-8.071-9.104-12.457c-3.921-5.725-7.437-11.323-10.577-16.757c3.141-5.434,6.656-11.032,10.578-16.757 c3.004-4.386,6.048-8.531,9.104-12.457c4.92-0.287,10.044-0.44,15.361-0.44c5.21,0,10.345,0.163,15.381,0.465 c3.05,3.919,6.087,8.055,9.085,12.432C155.887,116.468,159.402,122.066,162.543,127.5z M75.942,52.228 c1.346-0.922,3.046-1.37,5.197-1.37c8.105,0,21.021,6.607,35.502,19.272c-4.091,4.251-8.16,8.833-12.164,13.711 c-10.37,0.902-20.289,2.403-29.439,4.475C69.437,69.384,70.335,56.068,75.942,52.228z M90.66,102.267 c-2.215,3.233-4.333,6.489-6.368,9.753c-1.557-3.266-2.961-6.441-4.223-9.522c3.762-0.808,7.742-1.53,11.929-2.152 C91.552,100.986,91.103,101.62,90.66,102.267z M91.998,154.654c-4.187-0.622-8.167-1.343-11.929-2.151 c1.262-3.081,2.667-6.256,4.223-9.522c2.036,3.264,4.153,6.52,6.368,9.753C91.102,153.38,91.552,154.015,91.998,154.654z  M65.525,148.64c-18.784-6.078-29.261-14.343-29.261-21.14c0-6.797,10.477-15.062,29.262-21.14c2.738,6.898,6.037,13.99,9.828,21.14 C71.562,134.65,68.263,141.742,65.525,148.64z M81.139,204.142c-2.152,0-3.852-0.448-5.197-1.37 c-5.607-3.841-6.505-17.156-0.905-36.088c9.151,2.072,19.069,3.573,29.44,4.475c4.004,4.879,8.073,9.46,12.164,13.711 C102.16,197.536,89.245,204.143,81.139,204.142z M179.059,202.772c-1.346,0.922-3.046,1.37-5.198,1.37 c-8.105,0-21.021-6.607-35.502-19.272c4.09-4.251,8.16-8.832,12.164-13.711c10.371-0.902,20.289-2.402,29.44-4.475 C185.563,185.616,184.666,198.931,179.059,202.772z M218.736,127.5c0,6.797-10.477,15.061-29.261,21.14 c-2.738-6.898-6.037-13.99-9.828-21.14c3.777-7.124,7.065-14.19,9.797-21.065c1.79,0.58,3.546,1.178,5.232,1.813 C209.517,113.832,218.736,121.21,218.736,127.5z"/></svg> 
                        <span>Scholar</span></a></li>
                    <li><a href="mailto:&#110;&#105;&#99;&#111;&#108;&#97;&#115;&#46;&#97;&#117;&#100;&#101;&#98;&#101;&#114;&#116;&#64;&#99;&#110;&#97;&#109;&#46;&#102;&#114;">
                        <svg class="icon" role="img" enable-background="new 0 0 48 48" viewBox="0 0 48 48" xmlns="http://www.w3.org/2000/svg"><g id="Expanded"><g><g><path d="M44,40H4c-2.206,0-4-1.794-4-4V12c0-2.206,1.794-4,4-4h40c2.206,0,4,1.794,4,4v24C48,38.206,46.206,40,44,40z M4,10     c-1.103,0-2,0.897-2,2v24c0,1.103,0.897,2,2,2h40c1.103,0,2-0.897,2-2V12c0-1.103-0.897-2-2-2H4z"/></g><g><path d="M24,29.191L6.457,17.84c-0.464-0.301-0.597-0.919-0.297-1.383s0.919-0.596,1.383-0.297L24,26.809L40.457,16.16     c0.464-0.299,1.083-0.167,1.383,0.297s0.167,1.082-0.297,1.383L24,29.191z"/></g><g><path d="M6.001,34c-0.323,0-0.641-0.156-0.833-0.445c-0.307-0.46-0.183-1.08,0.277-1.387l9-6c0.46-0.307,1.081-0.183,1.387,0.277     c0.307,0.46,0.183,1.08-0.277,1.387l-9,6C6.384,33.945,6.191,34,6.001,34z"/></g><g><path d="M41.999,34c-0.19,0-0.383-0.055-0.554-0.168l-9-6c-0.46-0.307-0.584-0.927-0.277-1.387     c0.306-0.46,0.926-0.584,1.387-0.277l9,6c0.46,0.307,0.584,0.927,0.277,1.387C42.64,33.844,42.322,34,41.999,34z"/></g></g></g></svg>
                        <span>Email</span></a></li>
                </ul>
            </section>
        </aside>

        <div class="content">
                <a class="invisible" id="research"></a>
                <a class="invisible" id="teaching"></a>
                <a class="invisible" id="code"></a>
                <a class="invisible" id="publications"></a>
                <a class="invisible" id="news"></a>
                <a class="invisible" id="resume"></a>
        <section id="bio">
	    <h2>About me</h2>
            I am a computer vision and machine learning researcher interested in representation learning, multimodal learning & Earth Observation.
	    I am currently an assistant professor at <abbr title="Conservatoire National des Arts & Métiers">CNAM</abbr> in the <a href="http://cedric.cnam.fr/lab/equipes/vertigo/">Vertigo team</a>.
        </section>
        
        <section class="news">
            <h2>News</h2>
	    <p><strong>September 2021:</strong> I was distinguished as one of the <a href="http://iccv2021.thecvf.com/outstanding-reviewers">outstanding reviewers of ICCV 2021</a>.</p>
	    <p><strong>July 2021:</strong> <a href="https://sites.google.com/site/charpelletier/">Charlotte Pelletier</a> and I received a 7k€ grant for a project on super-resolution of Sentinel-2 time series from <a href="http://gdr-isis.ensea.fr">GDR ISIS</a>.</p>
	    <p><strong>July 2021:</strong> 2 papers accepted just before the holidays:
	    <ul>
		    <li> 1 paper to ISMIR21: <a href="https://arxiv.org/abs/2107.14009">PKSpell</a>, our deep recurrent network for automated pitch spelling and key signature estimation from MIDI musical performances.</li>
		    <li> 1 paper to ECML/PKDD Graph Embedding and Mining Workshop: <a href="https://arxiv.org/abs/2108.11629">Web Image Context Extraction</a> using graph neural networks on the HTML DOM tree.</li>
	    </ul>
	    </p>
	    <p><strong>December 2020:</strong> <span class="strike">The RL & Games project at CEDRIC laboratory is looking for a MSC. level intern for 5/6 months starting spring 2021 on producing diverse behaviours for AI using reinforcement learning in video games. See the <a href="files/2021_stage_rl_jeu.pdf">internship offer</a> (in French) for more details.</span> (this position has been filled)</p>
	    <p><strong>November 2020:</strong> <span class="strike">The Vertigo team is looking for a MSc. level intern for 5/6 months starting spring 2021 on equivariant neural networks for image classification and semantic segmentation. See the <a href="files/2021_stage_equivariance.pdf">internship offer</a> (in French) for more information</span>. (this position has been filled)</p>
	    <p><strong>October 2020:</strong> I was invited to organize a hands-on tutorial on machine learning for astrophysics at the  <a href="https://sftools-bigdata.sciencesconf.org/">SFtools-bigdata</a> conference. Code is available <a href="https://github.com/nshaud/ml_for_astro/blob/main/Hands_on_ML_for_astrophysics.ipynb">on my Github page</a>.</p>
	    <p><Strong>October 2020:</strong> A new specialization certificate on Artificial Intelligence has opened at Cnam. This program is tailored for professionnals that want to deepen their understanding of statistical learning, artificial intelligence and deep learning. Classes are tought either remotely or in the evening. Check out the curriculum <a href="https://formation.cnam.fr/rechercher-par-discipline/certificat-de-specialisation-intelligence-artificielle-1176377.kjsp">here (in French)</a>.</p>
	    <p><strong>June 2020:</strong> <span class="strike">I am looking for a PhD candidate for a fully-funded thesis on controlling generative networks for image synthesis in collaboration with CEA. Check out the <a href="http://cedric.cnam.fr/~crucianm/src/these_generative.pdf">full offer</a> fore more details.</span> This position has been filled.</p>
	    <p><strong>March 2020:</strong> We released <a href="https://ieee-dataport.org/open-access/sen12-flood-sar-and-multispectral-dataset-flood-detection">SEN12-FLOOD</a>, a flood detection dataset in Sentinel 1 and 2 time series.
	    <p><strong>January 2020:</strong> <span class=strike>Our laboratory is hiring a <a href="https://cedric.cnam.fr/lab/poste-mcf-ia-2020/">junior assistant professor</a> in Computer Science with a strong focus on machine learning and artificial intelligence. Application is done through the GALAXIE portal. It is a teaching and research permanent position in Paris.</span> Position has been filled.</p>
	    <p><strong>November 2019:</strong> We are looking to hire M2 interns from 5/6 months starting in spring 2020 on topics related to deep learning for image understanding. <span class="strike">I personally offer one internship on weakly-supervised semantic segmentation. Check out <a href="/static/2019_internship.pdf">the internship details</a></span>.
	    <span class="strike">I am also involved in another M2 internship offer on deep learning for MIMO radiocommunication (see the <a href="/static/2020_mimo_internship.pdf">subject</a> in French)</span> (these positions have been filled).
	    <span class="strike">Finally, I will co-advise with <a href="https://qwant.com">Qwant Research</a> a M2 internship on deep learning for fast webpage information extraction.</span>
	    If this interests you and feel free to contact me for more information.</p>
                <p><strong>September 2019:</strong> I am joining the <em>Conservatoire National des Arts & Métiers</em> (CNAM) as an assistant professor in the Vertigo team.</p>
                <p><strong>August 2019:</strong> Our journal article on using signed distance transform regression to regularize semantic segmentation deep networks has been accepted for publication in CVIU.</p>
                <p><strong>July 2019:</strong> I will be at <a href="https://www.irit.fr/pfia2019/apia/">APIA 2019</a> in Toulouse from July 1st to July 5th to present our work on multi-modal text/image classification with deep nets. Feel free to come for a chat!</p>
                <p><strong>May 2019:</strong> Our paper on multi-modal text/image deep networks for document image classification has been accepted to <a href="https://www.irit.fr/pfia2019/apia/">APIA 2019</a> in Toulouse.</p>
                <p><strong>April 2019:</strong> I will be presenting at the <a href="http://www.gdr-isis.fr/index.php?page=reunion&idreunion=386">GdR ISIS meeting</a> on weakly and semi-supervised learning for image and video classification. My talk will detail some of the work I did at Quicksign on image/text clustering for document recognition.</p>
                <p><strong>April 2019:</strong> Our review on deep convolutional and recurrent neural networks for hyperspectral image classification has ben accepted for the IEEE Geoscience and Remote Sensing special issue on hyperspectral data. Preprint <a href="https://hal.archives-ouvertes.fr/hal-02104998">here</a>.</p>
                <p><strong>January 2019:</strong> I joined Quicksign R&D team as a research scientist.</p>
                <p><strong>October 2018:</strong> I successfully defended my PhD thesis! The manuscript (in french) is available <a href="/files/Manuscrit.pdf">here</a> with <a href="/files/planches_soutenance.pdf">slides</a>.</p>
                <p><strong>July 2018:</strong> I was at IGARSS'18 in Valencia, where I presented our work on <a href="https://arxiv.org/abs/1806.02583">generative adversarial network for hyperspectral samples synthesis</a>. You can find the code <a href="http://github.com/nshaud/HyperGANs">here</a>!</p>
                <p><strong>March 2018:</strong> We have one paper accepted for <a href="https://igarss2018.org/">IGARSS 2018</a> on generative adversarial networks for hyperspectral data synthesis. We'll also appear on the <a href="https://project.inria.fr/aerialimagelabeling/">Inria Aerial Image Labeling</a> benchmark write-up on building extraction.</p>
                <p><strong>January 2018:</strong> I ported the code of our deep network for aerial/satellite semantic segmentation to PyTorch for an easier use: <a href="https://github.com/nshaud/DeepNetsForEO">fork it on GitHub</a>!</p>
                <p><strong>November 2017:</strong> Our latest journal paper on data fusion for remote sensing data using deep fully convolutional networks is <a href="https://hal.archives-ouvertes.fr/hal-01636145">out</a> !</p>
                <p><strong>July 2017:</strong> I was at CVPR 2017 for the Earthvision workshop, where I presented <a href="https://nicolas.audebert.at/blog/pdfs/AudebertLeSauxLefevre_Earthvision_joint_poster.pdf">our work</a> on semantic mapping using deep nets and OpenStreetMap data.</p>
                <p><strong>June 2017:</strong> I collaborated with the <a href="http://www.polytech.univ-smb.fr/index.php?id=listic-accueil&L=1">LISTIC team</a> on using deep nets to perform semantic segmentation on Sentinel-2 images. This work will be presented at <a href="http://www.igarss2017.org/Papers/viewpapers.asp?papernum=1919">IGARSS'17</a> in Forth Worth, Texas.</p>
                <p><strong>June 2017:</strong> I presented at ORASIS 2017 our work on data fusion with deep networks for remote sensing (<a href="https://nicolas.audebert.at/blog/pdfs/AudebertLeSauxLefevre_ORASIS_fusion.pdf">slides</a>).</p>
                <p><strong>May 2017:</strong> Our submission on <a href="https://nicolas.audebert.at/blog/pdfs/AudebertLeSauxLefevre_CVPRW17_JointLearning.pdf">joint deep learning using optical and OSM data</a> for semantic mapping of aerial/satellite images has been accepted to the EarthVision 2017 CVPR Workshop !</p>
                <p><strong>April 2017:</strong> Our Remote Sensing journal paper on <a href="http://www.mdpi.com/2072-4292/9/4/368">vehicle segmentation for detection and classification</a> is out in open access on the MPDI website.</p>
                <p><strong>March 2017:</strong> My colleague <a href="https://sites.google.com/view/boulch">Alexandre Boulch</a> will present the SnapNet architecture for semantic segmentation of unstructured point clouds at Eurographics 3DOR workshop. It is the current state-of-the-art on the <a href="http://semantic3d.net">Semantic3D</a> dataset (<a href="https://github.com/aboulch/snapnet">code</a>).</p>
                <p><strong>March 2017:</strong> Our paper on <a href="https://nicolas.audebert.at/blog/pdfs/audebert_lesaux_lefevre_data_fusion_for_urban_remote_sensing.pdf">data fusion for remote sensing using deep nets</a> won the 2nd best student paper award at JURSE 2017 ! <a href="blog/pdfs/AudebertLeSauxLefevre_DataFusion-Slides.pdf">Slides</a> and <a href="blog/pdfs/AudebertLeSauxLefevre_DataFusion-Poster.pdf">poster</a> are available.</p>
                <p><strong>Februrary 2017:</strong> The code of the deep network we used for the ISPRS Vaihingen 2D Semantic Labeling Challenge is out on <a href="https://github.com/nshaud/DeepNetsForEO">Github</a> !<p>
                <p><strong>January 2017:</strong> We will present two invited papers at <a href="http://www.jurse2017.com/">JURSE 2017</a> !</p>
                <p><strong>November 2016:</strong> I will be at <a href="http://www.accv2016.org">ACCV'16</a> in Taipei to present our poster on semantic segmentation of Earth Observation using multi-scale and multimodal deep networks.</p>
                <p><strong>October 2016:</strong> I will be at <a href="https://2016.pycon.fr/">PyCon-fr</a> (the French Python conference) to speak about deep learning using Python (<a href="https://nicolas.audebert.at/blog/pdfs/pyconfr2016-deep-learning-naudebert.pdf">slides (in French)</a> and <a href="https://www.pycon.fr/2016/videos/deep-learning-votre-propre-cerveau-artificiel-avec-python.html">video (in French, too)</a>).</p> 
                <p><strong>September 2016:</strong> Our paper on the use of deep networks for object-based image analysis of vehicles in the ISPRS dataset has been distinguished by the "<em>Best Benchmarking Contribution Award</em>" at <a href="https://www.geobia2016.com/">GEOBIA 2016</a> !</p>
                <p><strong>September 2016:</strong> I will be at <a href="https://www.geobia2016.com/">GEOBIA 2016</a> in Enschede to talk about our work on object-based analysis of cars in remote sensing images using deep learning.</p>
                <p><strong>September 2016:</strong> Our paper on semantic segmentation for Earth Observation was accepted at <a href="http://accv2016.org">ACCV'16</a> for a poster presentation. Check out the state-of-the-art results on the <a href="http://www2.isprs.org/vaihingen-2d-semantic-labeling-contest.html">ISPRS Vaihingen 2D Semantic Labeling Challenge</a> !</p>
                <p><strong>July 2016:</strong> I will be at <a href="http://igarss2016.org/">IGARSS'16</a> in Beijing to present our work on superpixel-based semantic segmentation of aerial images.</p>
                <p><strong>April 2016:</strong> Our paper on region-based classification of remote sensing images using deep features has been accepted at <a href="http://igarss2016.org/">IGARSS'16</a> for an oral presentation.</p>
                <p><strong>October 2015:</strong> I started as a PhD student at ONERA and IRISA.</p>
        </section>
        
        <section class="research">
            <h2>Research areas</h2>
	    My research interests shifts with time but my work falls broadly into one of these three areas which overlap - more often than not.

            <h3>Machine learning</h3>

            Machine learning consists in training a computer to perform a task without explicitly detailing how to do so, but instead by creating models that leverage patterns in the data and perform inference from them.
            Especially interesting is <em>deep learning</em>, i.e. producing abstract representation of raw data that allows the computer to manipulate high-level concepts in numerical terms.
	    I am mostly interested in how machine learning can improve machine perception (image, sound and text processing and understanding) through useful <em>representations</em>.

            <h3>Earth Observation</h3>

            Remote sensing of Earth Observation data is a very broad field that aims to gather information about our planet and the objects that cover its surface.
            Earth Observation leverages satellite images (in the broad sense that radar, multispectral and infrared data are also images) to understand ecological phenomenon, map urban expansions or monitor natural disasters.
            My work focuses on processing and making sense of the large volumes of data acquired by the sensors for land cover mapping, change detection and image interpreation through automated means.

            <h3>Data fusion</h3>

            Information processing in the modern age constrain computers to work with a large panel of data types: images, videos, texts, sounds, Lidar, radar, sonar and other various kinds of sensors.
            In the real world, humans are able to sense the world through multiple modalities by analog means.
            Data fusion aims to reproduce these abilities in the digital world for cross-modal data mining and pattern recognition, e.g. making sense of complex manuscripts or understanding movie clips.

	    <h3>Machine learning and games</h3>

	    I have recently picked up machine learning for games as an interest. I lead a small project at Cnam on improving reinforcement learning to generate diverse and challenging AI in video games (collab. with Clément Rambour and Guillaume Levieux).
	    Our goal is to better understand what makes an interesting AI or environment from a gameplay point of view. My focus is on how to use machine learning in clever ways to improve the gaming experience.

            <h2>Collaborators</h2>

	    I am sometimes looking for collaborators (either at masters or PhD level). You can cold email me but check that my current research is interesting to you and is something that you feel comfortable working with.

	    <h3>PhD students</h3>

	    I currently advise two PhD students:
	    <ul>
		    <li><a href="https://cedric.cnam.fr/lab/author/pdoubinsky/">Perla Doubinsky</a> (Cnam/CEA): controlling generative models to steer image generation, with Michel Crucianu (Cnam) and Hervé Le Borgne (CEA) since November 2020.</li>
		    <li><a href="https://cedric.cnam.fr/lab/author/eramzi/">Elias Ramzi</a> (Cnam/SWORD): deep learning for visual search in large logos databases, with Nicolas Thome (Cnam), Clément Rambour (Cnam) and Xavier Bitot (SWORD Group) since January 2021.</li>
	    </ul>

	    <h3>Postdocs</h3>

            From september 2020 to september 2021, I am working with <a href="https://ipag.osug.fr/~robitaij/index.html">Jean-François Robitaille</a> from IPAG on deep learning multispectral image processing to detect multiscale interstellar structures. Our goal is to use data mining to detect interstellar objects in large clouds.

	    <h4>Past</h4>
	    From september 2019 to september 2020, I worked with <a href="https://cedric.cnam.fr/lab/author/crambour">Clément Rambour</a> (CNAM/ONERA) during his post-doc on multi-temporal analysis of SAR and optical data. We focused our work on flood detection in SAR/multispectral time series.

            <h3>MSc. students</h3>

	    <h4>Current</h4>
	    <!-- I am not supervising any master student currently. -->

	    I am currently advising two students for their master internship:
	    <ul>
		    <li><a href="https://egiob.github.io/markdown-cv/">Raphaël Boige</a> (master Data Science, Télécom Paris/École Polytechnique). Internship on learning agents with diversified behaviours for video games by reinforcement.</li>
		    <li><a href="https://fr.linkedin.com/in/jo%C3%A3o-pedro-ara%C3%BAjo-ferreira-campos-bb14931a9">João Pedro Araújo Ferreira Campos</a> (ENSTA Paris). Internship on spatially-equivariant representation learning for remote sensing and medical images.</li>
	    </ul>

	    <h4>Past</h4>

	    I have supervised or co-supervised several students for their research internship:
            <ul>
		    <li><a href="https://scholar.google.com/citations?user=P75u0xEAAAAJ">Javiera Navarro-Castillo</a> (ONERA): Towards the ``ImageNet'' of remote sensing, MSc. from École Polytechnique, 2018. Internship on large-scale semi-supervised semantic segmentation (with A. Boulch, B. Le Saux and S. Lefèvre). Now a PhD student at ONERA.</li>
                <li><a href="https://fr.linkedin.com/in/adel-redjimi-05183713a">Adel Redjimi</a> (Quicksign): Semi-supervised learning for document image classification, MSc. from INP Grenoble, 2019. Internship on document classification with scarce supervision and unlabeled data (with K. Slimani). Now a PhD student at Cnam/CEA.</li>
		<li><a href="https://fr.linkedin.com/in/chendang">Chen Dang</a> (Qwant) : fast webpage information extraction. MSc. from Université Paris Sciences & Lettres, 2020. Internship on webpage image context extraction for indexing (with R. Fournier-S'niehotta and H. Randrianarivo). Now a PhD student at Orange Labs.</li>
		<li><a href="https://tn.linkedin.com/in/samar-chebbi">Samar Chebbi</a> (Cnam) : deep learning for linear precoding in massive MIMO systems with power amplifiers, Sup'com engineering diploma, 2020 (with R. Zayani and M. Ferecatu).</li>
		<li><a href="https://fr.linkedin.com/in/yannis-karmim-59257b18b">Yannis Karmim</a> (Cnam) : semi-supervised learning for semantic segmentation, MSC. from Sorbonne Universités, 2020. Internship on equivariance constraints for segmentation models (with N. Thome). Now a research engineer at Cnam.</li>
            </ul>
        </section>

        <section class="code">
            <h2>Code & datasets</h2>
            Most of my research code is released under permissive open source licenses on <a href="https://github.com/nshaud">GitHub</a>.

            During my time at Quicksign, we released <a href="https://github.com/Quicksign/ocrized-text-dataset">QS-OCR</a>, a text/image classification dataset using OCR'd document images.

	    Improving on the 2019 MediaEval Multimedia Satellite Task, we designed and released in 2020 <a href="https://clmrmb.github.io/SEN12-FLOOD/">SEN12-FLOOD</a>, a multimodal SAR/multispectral dataset (Sentinel 1 and 2) for flood event detection in remote sensing tim
e series.
        </section>

        <section class="publications">
            <h2>Publications</h2>

            <h3 id="journals">Journals:</h3>
            <ul>
		<li>2021, <em>Semi-Supervised Semantic Segmentation in Earth Observation: The MiniFrance Suite, Dataset Analysis and Multi-task Network Study</em>, Javiera Castillo-Navarro, Bertrand Le Saux, Alexandre Boulch, <strong>Nicolas Audebert</strong>, Sébastien Lefèvre, Machine Learning Journal (in press).</li>
                <li>2019, <em>Deep learning for classification of hyperspectral data: a comparative review</em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, IEEE Geosciences and Remote Sensing Magazine.</li>
		<li>2019, <em>Distance transform regression for spatially-aware deep semantic segmentation</em>, <strong>Nicolas Audebert</strong>, Alexandre Boulch, Bertrand Le Saux, Sébastien Lefèvre, Computer Vision and Image Understanding.</li>
                <li>2018, <em>Beyond RGB: Very High Resolution Urban Remote Sensing With Multimodal Deep Networks</em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, ISPRS Journal of Photogrammetry and Remote Sensing, Elsevier, 2018.</li>
                <li>2017, <em>Segment-before-Detect: Vehicle Detection and Classification through Semantic Segmentation of Aerial Images</em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, Remote Sensing, MDPI, 2017.</li>
                </ul>
                <h3 id="conferences">Conferences:</h3>
                <ul>
		<li>2021, <em>PKSpell: Data-Driven Pitch Spelling and Key Signature Estimation</em>, Francesco Foscarin, <strong>Nicolas Audebert</strong>, Raphaël Fournier-S'Niehotta, ISMIR, virtual.</li>
		<li>2021, <em>Web Image Context Extraction with Graph Neural Networks and Sentence Embeddings on the DOM tree</em>, Chen Dang, Hicham Randrianarivo, Raphaël Fournier-S'Niehotta, <strong>Nicolas Audebert</strong>, ECML/PKDD: GEM workshop, virtual.</li>
		<li>2020, <em>A Real-World Hyperspectral Image Processing Workflow for Vegetation Stress and Hydrocarbon Indirect Detection</em>, Dominique Dubucq, <strong>Nicolas Audebert</strong>, Véronique Achard, Alexandre Alakian, Sophie Fabre, Anthony Credoz, Philippe Deliot, Bertrand Le Saux, XXIV ISPRS Congress, Nice.</li>
		<li>2020, <em>Flood Detection in Time Series of Optical and SAR images</em>, Clément Rambour, <strong>Nicolas Audebert</strong>, Elise Koeniguer, Bertrand Le Saux, Michel Crucianu, Mihai Datcu, XXIV ISPRS Congress, Nice.</li>
                <li>2019, <em>Multimodal deep networks for text and image-based document classification</em>, <strong>Nicolas Audebert</strong>, Catherine Herold, Kuider Slimani, Cédric Vidal, ECML/PKDD Workshop on Multi-view Learning, Würzburg.</li>
                <li>2018, <em>Generative Adversarial Networks for Realistic Synthesis of Hyperspectral Samples</em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, IGARSS, Valencia, 2018.</li>
                <li>2017, <em>Couplage de données géographiques participatives et d’images aériennes par apprentissage profond</em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, GRETSI, Juan-les-Pins, 2017.</li>
                <li>2017, <em>Joint Learning from Earth Observation and OpenStreetMap Data to Get Faster Better Semantic Maps</em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, EarthVision - CVPR Workshop, Honolulu, 2017 (poster).</li>
                <li>2017, <em>Unstructured point cloud semantic labeling using deep segmentation networks</em>, Alexandre Boulch, Bertrand Le Saux, <strong>Nicolas Audebert</strong>, Eurographics 3DOR, Lyon, 2017.</li>
                <li>2017, <em>Fusion of Heterogeneous Data in Convolutional Networks for Urban Semantic Labeling</em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, JURSE, Dubai, 2017 (slides, poster).</li>
                <li>2016, <em>Semantic Segmentation of Earth Observation Data Using Multimodal and Multi-scale Deep Networks</em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, ACCV, Taipei, 2016 (poster).</li>
                <li>2016, <em>On the usability of deep networks for object-based image analysis</em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, GEOBIA, Enschede, 2016 (slides).</li>
                <li>2016, <em>How useful is region-based classification of remote sensing images in a deep learning framework ?</em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, IGARSS, Beijing, 2016 (slides).</li>
                <li>2016, <em>Structural classifiers for contextual semantic labeling of aerial images</em>. Hicham Randrianarivo, Bertrand Le Saux, <strong>Nicolas Audebert</strong>, Michel Crucianu, Marin Ferecatu, Big Data from Space (BiDS), Tenerife, 2016.</li>
                </ul>
                <h3 id="communications">Communications :</h3>
                <ul>
                <li>2019, <em>Multimodal deep networks for text and image-based document classification</em>, <strong>Nicolas Audebert</strong>, Catherine Herold, Kuider Slimani, Cédric Vidal, APIA (Applications Pratiques de l’Intelligence Artificielle), Toulouse, 2019 (slides).</li>
                <li>2017, <em>Réseaux de neurones profonds et fusion de données pour la segmentation sémantique d’images aériennes</em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, ORASIS 2017 (Journées des jeunes chercheurs en vision par ordinateur), Colleville-sur-Mer, 2017 (slides).</li>
                <li>2016, <em>Deep Learning for Remote Sensing</em>. <strong>Nicolas Audebert</strong>, Alexandre Boulch, Adrien Lagrange, Bertrand Le Saux, Sébastien Lefèvre, 16th ONERA-DLR Aerospace Symposium (ODAS), Oberpfaffenhofen, 2016.</li>
                <li>2016, <em>Deep learning for aerial cartography</em> (poster). <strong>Nicolas Audebert</strong>, Bertrand Le Saux, Sébastien Lefèvre, Statlearn Workshop, Vannes, 2016.</li>
            </ul>

        </section>

        <section class="teaching">
            <h2>Teaching</h2>

            <h3>Current classes</h3>
            I mainly teach pattern recognition, image processing and machine learning for text and image understanding in CNAM:
	    <ul>
		    <li><a href="http://cedric.cnam.fr/vertigo/Cours/ml/">RCP208</a>: Pattern Recognition (practical classes).</li>
		    <li><a href="https://cedric.cnam.fr/vertigo/Cours/ml2/">RCP209</a>: Supervised Learning and Neural Networks (intro and practical classes).</li>
		    <li><a href="https://cedric.cnam.fr/vertigo/Cours/RCP211/">RCP211</a>: Advanced Artificial Intelligence (generative models).</li>
		    <li><a href="https://cedric.cnam.fr/vertigo/Cours/RCP217/">RCP217</a>: Artificial Intelligence for Multimedia Applications (deep learning for audio processing).</li>
		    <li><a href="https://cedric.cnam.fr/vertigo/Cours/RCP216/">RCP216</a>: Large Scale Machine Learning with Spark.</li>
	    </ul>

            <h3>Past courses</h3>
            In the past I have tought <a href="http://imagine.enpc.fr/~monasse/Info/">"Introduction to C++ programming"</a> and <a href="http://imagine.enpc.fr/~monasse/Algo/">"Algorithms"</a> classes at École Nationale des Ponts et Chaussées.
            Some resources are still available in the <a href="/legacy/teaching/ENPC/">archive</a>.
        </section>

        <section class="resume">
            <h2>Resume</h2>
            <h3>Short vitae</h3>

            <ul>
                <li>Since Sept. 2019: Assistant Professor in Computer Science at <em>Conservatoire National des Arts & Métiers</em> (CNAM).</li>
                <li>Jan. 2019 - Aug. 2019: Research scientist in deep learning and computer vision at <a href="https://quicksign.com">Quicksign</a>.</li>
                <li>Oct. 2015 - Oct. 2018: PhD in Computer Science at <a href="https://www.onera.fr"><abbr title="Office national d'études et de recherche en aérospatial">ONERA</abbr></a> and <a href="https://www.irisa.fr"><abbr title="Institut de recherche en informatique et systèmes aléatoires">IRISA</abbr></a>.</li>
                <li>2015: MEng. in Computer Science from Supélec.</li>
                <li>2015: MSc. in Human-Computer Interaction from Université Paris-Sud.</li>
            </ul>

            <h3>Community service</h3>

            <ul>
                <li>Reviewer for ICCV, CVIU, BMVC, IEEE JSTARS, IEEE TGRS, IEEE TIP, ISPRS Journal of Photogrammetry and Remote Sensing, MDPI Remote Sensing… See my reviewer record on <a href="https://publons.com/researcher/3038492/nicolas-audebert/peer-review/">Publons</a>.</li>
		<li>Program committee member for <a href="https://www.grss-ieee.org/earthvision2019/people.html">EarthVision 2019</a>, <a href="https://www.grss-ieee.org/earthvision2020/people.html">EarthVision 2020</a>, <a href="http://www.classic.grss-ieee.org/earthvision2021/">EarthVision 2021</a> (CVPR Workshop), <a href="https://mdl4eo.irstea.fr/maclean-machine-learning-for-earth-observation/">MACLEAN 2019</a>, <a href="https://sites.google.com/view/maclean2019/">MACLEAN 2020</a> and <a href="https://sites.google.com/view/maclean21/">MACLEAN 2021</a> (ECML/PKDD Workshop).</li>
            </ul>
        </section>

        <section class="misc">
            <h2>Misc</h2>
                
	    <p>I am an occasional translator for the <a href="https://github.com/python/python-docs-fr">French Python documentation project</a>.</p>

	    <p>I have an <a href="https://en.wikipedia.org/wiki/Erd%C5%91s_number">Erdős number</a> of <a href="https://www.csauthors.net/distance/paul-erdos/nicolas-audebert">4</a>. I don't have an <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Bacon_number">Erdős–Bacon number</a> yet but you never know…
	    </p>

            My <a href="https://en.wikipedia.org/wiki/Kardashian_Index">Kardashian index</a> is 0.61.
        </section>
        </div>
        </div>

        <footer>
            Nicolas Audebert - <a href="https://creativecommons.org/licenses/by-nc/2.0/fr/deed.en">CC-BY-NC 2.0</a> - 2021
        </footer>

    </body>
</html>
