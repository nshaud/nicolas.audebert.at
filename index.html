<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Nicolas Audebert - Computer Vision & Deep Learning Researcher</title>
        <meta charset="UTF-8" />
        <meta name="referrer" content="none" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
	<meta http-equiv="Content-Security-Policy" content="script-src 'self'">
        <link href="static/css/audebert.at.css" rel="stylesheet">
    </head>

    <body>
        <header>
        </header>

        <nav>
            <h3 class="navlink" id="sitename"><a href="">Nicolas Audebert</a></h3>
            <h3 class="navlink"><a href="#news">News</a></h3>
            <h3 class="navlink"><a href="#projects">Projects</a></h3>
            <h3 class="navlink"><a href="#research">Research</a></h3>
            <h3 class="navlink"><a href="#code">Code/datasets</a></h3>
            <h3 class="navlink"><a href="#publications">Publications</a></h3>
            <h3 class="navlink"><a href="#teaching">Teaching</a></h3>
            <h3 class="navlink"><a href="#resume">Resume</a></h3>
        </nav>
        <div class="flex-container">

        <aside>
            <div class="picture"><img src="static/img/na2.jpg" width=150 height=150 alt="Profile picture" /></div>
            <h1>Nicolas Audebert</h1>
            <section id="about">
                <h2>Computer Vision and Machine Learning researcher</h2>
		<h2>Junior research director ‚Äî AI chair @ IGN</h2>
		<h2 style="font-size: small; margin-bottom: 0px;">Associate Professor of Computer Science (<em>ma√Ætre de conf√©rences</em>)</h2>
                <h2 style="font-size: small; margin-top: 0px;">On leave from Conservatoire national des arts et m√©tiers (Cnam)</h2>
                <ul>
                    <li><a href="https://github.com/nshaud">
                        <svg class="icon" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub icon</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg> 
                        <span>GitHub</span></a></li>
                    <li><a href="https://twitter.com/nshaud"><svg class="icon" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Twitter icon</title><path d="M23.954 4.569c-.885.389-1.83.654-2.825.775 1.014-.611 1.794-1.574 2.163-2.723-.951.555-2.005.959-3.127 1.184-.896-.959-2.173-1.559-3.591-1.559-2.717 0-4.92 2.203-4.92 4.917 0 .39.045.765.127 1.124C7.691 8.094 4.066 6.13 1.64 3.161c-.427.722-.666 1.561-.666 2.475 0 1.71.87 3.213 2.188 4.096-.807-.026-1.566-.248-2.228-.616v.061c0 2.385 1.693 4.374 3.946 4.827-.413.111-.849.171-1.296.171-.314 0-.615-.03-.916-.086.631 1.953 2.445 3.377 4.604 3.417-1.68 1.319-3.809 2.105-6.102 2.105-.39 0-.779-.023-1.17-.067 2.189 1.394 4.768 2.209 7.557 2.209 9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63.961-.689 1.8-1.56 2.46-2.548l-.047-.02z"/></svg> 
                        <span>Twitter</span></a></li>
                    <li><a href="https://orcid.org/0000-0001-6486-3102">
                        <svg class="icon" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>ORCID icon</title><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"/></svg> 
                        <span>ORCID</span></a></li>
		    <li><a rel="me" href="https://mastodon.social/@nsh">
		  	<svg class="icon" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 216.4144 232.00976">
				  <path fill="#2b90d9" d="M211.80734 139.0875c-3.18125 16.36625-28.4925 34.2775-57.5625 37.74875-15.15875 1.80875-30.08375 3.47125-45.99875 2.74125-26.0275-1.1925-46.565-6.2125-46.565-6.2125 0 2.53375.15625 4.94625.46875 7.2025 3.38375 25.68625 25.47 27.225 46.39125 27.9425 21.11625.7225 39.91875-5.20625 39.91875-5.20625l.8675 19.09s-14.77 7.93125-41.08125 9.39c-14.50875.7975-32.52375-.365-53.50625-5.91875C9.23234 213.82 1.40609 165.31125.20859 116.09125c-.365-14.61375-.14-28.39375-.14-39.91875 0-50.33 32.97625-65.0825 32.97625-65.0825C49.67234 3.45375 78.20359.2425 107.86484 0h.72875c29.66125.2425 58.21125 3.45375 74.8375 11.09 0 0 32.975 14.7525 32.975 65.0825 0 0 .41375 37.13375-4.59875 62.915"/>
				  <path fill="#fff" d="M177.50984 80.077v60.94125h-24.14375v-59.15c0-12.46875-5.24625-18.7975-15.74-18.7975-11.6025 0-17.4175 7.5075-17.4175 22.3525v32.37625H96.20734V85.42325c0-14.845-5.81625-22.3525-17.41875-22.3525-10.49375 0-15.74 6.32875-15.74 18.7975v59.15H38.90484V80.077c0-12.455 3.17125-22.3525 9.54125-29.675 6.56875-7.3225 15.17125-11.07625 25.85-11.07625 12.355 0 21.71125 4.74875 27.8975 14.2475l6.01375 10.08125 6.015-10.08125c6.185-9.49875 15.54125-14.2475 27.8975-14.2475 10.6775 0 19.28 3.75375 25.85 11.07625 6.36875 7.3225 9.54 17.22 9.54 29.675"/>
			</svg>
			<span>Mastodon</span></a></li>
                    <li><a href="https://scholar.google.com/citations?user=_z5vXUcAAAAJ">
                        <svg class="icon" role="img" viewBox="0 0 255 255" xmlns="http://www.w3.org/2000/svg"><title>Google Scholar icon</title><path d="M127.5,0C57.196,0,0,57.196,0,127.5S57.196,255,127.5,255S255,197.804,255,127.5S197.804,0,127.5,0z M127.5,240 C65.467,240,15,189.533,15,127.5S65.467,15,127.5,15S240,65.467,240,127.5S189.533,240,127.5,240z M127.5,104.271 c-11.696,0-21.211,9.516-21.211,21.211s9.516,21.211,21.211,21.211c11.696,0,21.211-9.516,21.211-21.211 S139.196,104.271,127.5,104.271z M127.5,131.694c-3.425,0-6.211-2.787-6.211-6.211s2.787-6.211,6.211-6.211 c3.425,0,6.211,2.787,6.211,6.211S130.925,131.694,127.5,131.694z M194.351,92.261c0.567-1.933,1.101-3.849,1.561-5.728 c5.546-22.602,2.571-39.18-8.378-46.68c-3.87-2.651-8.471-3.995-13.674-3.995c-12.948,0-29.48,8.806-46.361,23.809 c-16.881-15.003-33.413-23.809-46.361-23.809c-5.203,0-9.804,1.344-13.674,3.995c-10.949,7.5-13.924,24.078-8.377,46.68 c0.461,1.879,0.994,3.795,1.562,5.729c-1.915,0.624-3.797,1.267-5.607,1.948c-21.782,8.196-33.778,20.019-33.778,33.29 c0,13.272,11.996,25.095,33.778,33.292c1.811,0.681,3.692,1.325,5.607,1.948c-0.567,1.933-1.101,3.849-1.561,5.728 c-5.547,22.602-2.572,39.18,8.378,46.681c3.87,2.65,8.47,3.994,13.673,3.994c0.001,0,0,0,0.001,0c12.947,0,29.48-8.805,46.36-23.808 c16.881,15.003,33.413,23.808,46.361,23.808c5.203,0,9.803-1.344,13.674-3.995c10.949-7.501,13.924-24.079,8.377-46.68 c-0.461-1.879-0.994-3.795-1.561-5.728c1.914-0.624,3.796-1.267,5.607-1.948c21.782-8.197,33.778-20.02,33.778-33.292 s-11.996-25.095-33.778-33.291C198.147,93.527,196.266,92.884,194.351,92.261z M173.861,50.857c2.151,0,3.851,0.448,5.197,1.37 c5.607,3.841,6.505,17.156,0.904,36.088c-9.15-2.072-19.068-3.572-29.439-4.474c-4.004-4.879-8.074-9.46-12.165-13.711 C152.839,57.465,165.755,50.857,173.861,50.857z M164.341,152.733c2.215-3.234,4.332-6.489,6.368-9.753 c1.556,3.266,2.96,6.441,4.223,9.522c-3.762,0.809-7.742,1.53-11.929,2.152C163.448,154.015,163.897,153.38,164.341,152.733z  M163.038,100.398c4.086,0.613,8.047,1.337,11.867,2.162c-1.256,3.061-2.651,6.216-4.197,9.459c-2.035-3.264-4.153-6.52-6.368-9.753 C163.909,101.637,163.472,101.021,163.038,100.398z M162.543,127.5c-3.141,5.434-6.656,11.032-10.577,16.757 c-3.004,4.386-6.048,8.531-9.105,12.457c-4.92,0.287-10.044,0.44-15.362,0.44c-5.317,0-10.44-0.153-15.361-0.44 c-3.056-3.926-6.101-8.071-9.104-12.457c-3.921-5.725-7.437-11.323-10.577-16.757c3.141-5.434,6.656-11.032,10.578-16.757 c3.004-4.386,6.048-8.531,9.104-12.457c4.92-0.287,10.044-0.44,15.361-0.44c5.21,0,10.345,0.163,15.381,0.465 c3.05,3.919,6.087,8.055,9.085,12.432C155.887,116.468,159.402,122.066,162.543,127.5z M75.942,52.228 c1.346-0.922,3.046-1.37,5.197-1.37c8.105,0,21.021,6.607,35.502,19.272c-4.091,4.251-8.16,8.833-12.164,13.711 c-10.37,0.902-20.289,2.403-29.439,4.475C69.437,69.384,70.335,56.068,75.942,52.228z M90.66,102.267 c-2.215,3.233-4.333,6.489-6.368,9.753c-1.557-3.266-2.961-6.441-4.223-9.522c3.762-0.808,7.742-1.53,11.929-2.152 C91.552,100.986,91.103,101.62,90.66,102.267z M91.998,154.654c-4.187-0.622-8.167-1.343-11.929-2.151 c1.262-3.081,2.667-6.256,4.223-9.522c2.036,3.264,4.153,6.52,6.368,9.753C91.102,153.38,91.552,154.015,91.998,154.654z  M65.525,148.64c-18.784-6.078-29.261-14.343-29.261-21.14c0-6.797,10.477-15.062,29.262-21.14c2.738,6.898,6.037,13.99,9.828,21.14 C71.562,134.65,68.263,141.742,65.525,148.64z M81.139,204.142c-2.152,0-3.852-0.448-5.197-1.37 c-5.607-3.841-6.505-17.156-0.905-36.088c9.151,2.072,19.069,3.573,29.44,4.475c4.004,4.879,8.073,9.46,12.164,13.711 C102.16,197.536,89.245,204.143,81.139,204.142z M179.059,202.772c-1.346,0.922-3.046,1.37-5.198,1.37 c-8.105,0-21.021-6.607-35.502-19.272c4.09-4.251,8.16-8.832,12.164-13.711c10.371-0.902,20.289-2.402,29.44-4.475 C185.563,185.616,184.666,198.931,179.059,202.772z M218.736,127.5c0,6.797-10.477,15.061-29.261,21.14 c-2.738-6.898-6.037-13.99-9.828-21.14c3.777-7.124,7.065-14.19,9.797-21.065c1.79,0.58,3.546,1.178,5.232,1.813 C209.517,113.832,218.736,121.21,218.736,127.5z"/></svg> 
                        <span>Scholar</span></a></li>
                    <li><a href="mailto:&#110;&#105;&#99;&#111;&#108;&#97;&#115;&#46;&#97;&#117;&#100;&#101;&#98;&#101;&#114;&#116;&#64;&#99;&#110;&#97;&#109;&#46;&#102;&#114;">
                        <svg class="icon" role="img" enable-background="new 0 0 48 48" viewBox="0 0 48 48" xmlns="http://www.w3.org/2000/svg"><g id="Expanded"><g><g><path d="M44,40H4c-2.206,0-4-1.794-4-4V12c0-2.206,1.794-4,4-4h40c2.206,0,4,1.794,4,4v24C48,38.206,46.206,40,44,40z M4,10     c-1.103,0-2,0.897-2,2v24c0,1.103,0.897,2,2,2h40c1.103,0,2-0.897,2-2V12c0-1.103-0.897-2-2-2H4z"/></g><g><path d="M24,29.191L6.457,17.84c-0.464-0.301-0.597-0.919-0.297-1.383s0.919-0.596,1.383-0.297L24,26.809L40.457,16.16     c0.464-0.299,1.083-0.167,1.383,0.297s0.167,1.082-0.297,1.383L24,29.191z"/></g><g><path d="M6.001,34c-0.323,0-0.641-0.156-0.833-0.445c-0.307-0.46-0.183-1.08,0.277-1.387l9-6c0.46-0.307,1.081-0.183,1.387,0.277     c0.307,0.46,0.183,1.08-0.277,1.387l-9,6C6.384,33.945,6.191,34,6.001,34z"/></g><g><path d="M41.999,34c-0.19,0-0.383-0.055-0.554-0.168l-9-6c-0.46-0.307-0.584-0.927-0.277-1.387     c0.306-0.46,0.926-0.584,1.387-0.277l9,6c0.46,0.307,0.584,0.927,0.277,1.387C42.64,33.844,42.322,34,41.999,34z"/></g></g></g></svg>
                        <span>Email</span></a></li>
                </ul>
            </section>
        </aside>

        <div class="content">
                <a class="invisible" id="research"></a>
                <a class="invisible" id="teaching"></a>
                <a class="invisible" id="projects"></a>
                <a class="invisible" id="code"></a>
                <a class="invisible" id="publications"></a>
                <a class="invisible" id="news"></a>
                <a class="invisible" id="resume"></a>
        <section id="bio">
	    <h2>üë§ About me</h2>
            I am a computer vision and machine learning researcher interested in representation learning, multimodal learning, Earth Observation and video games.
	    I am working as a junior research director (<em>directeur de recherches junior</em>) at <abbr title="National Institute of Geographic and Forest Information">IGN</abbr> in the <a href="https://www.umr-lastig.fr/">LASTIG laboratory, STRUDEL team</a>.
	    <p style="font-size: small;">I am currently on leave from my associate professor position at <abbr title="Conservatoire national des arts & m√©tiers">Cnam</abbr> in the <a href="http://cedric.cnam.fr/lab/equipes/vertigo/">Vertigo team</a>.</p>

	    <h3>Contact:</h3>

	    <ul style="list-style-type: none;">
		<li>
                     <svg class="icon" role="img" enable-background="new 0 0 48 48" viewBox="0 0 48 48" xmlns="http://www.w3.org/2000/svg"><g id="Expanded"><g><g><path d="M44,40H4c-2.206,0-4-1.794-4-4V12c0-2.206,1.794-4,4-4h40c2.206,0,4,1.794,4,4v24C48,38.206,46.206,40,44,40z M4,10     c-1.103,0-2,0.897-2,2v24c0,1.103,0.897,2,2,2h40c1.103,0,2-0.897,2-2V12c0-1.103-0.897-2-2-2H4z"/></g><g><path d="M24,29.191L6.457,17.84c-0.464-0.301-0.597-0.919-0.297-1.383s0.919-0.596,1.383-0.297L24,26.809L40.457,16.16     c0.464-0.299,1.083-0.167,1.383,0.297s0.167,1.082-0.297,1.383L24,29.191z"/></g><g><path d="M6.001,34c-0.323,0-0.641-0.156-0.833-0.445c-0.307-0.46-0.183-1.08,0.277-1.387l9-6c0.46-0.307,1.081-0.183,1.387,0.277     c0.307,0.46,0.183,1.08-0.277,1.387l-9,6C6.384,33.945,6.191,34,6.001,34z"/></g><g><path d="M41.999,34c-0.19,0-0.383-0.055-0.554-0.168l-9-6c-0.46-0.307-0.584-0.927-0.277-1.387     c0.306-0.46,0.926-0.584,1.387-0.277l9,6c0.46,0.307,0.584,0.927,0.277,1.387C42.64,33.844,42.322,34,41.999,34z"/></g></g></g></svg>
		     Email: 
                     <!-- CNAM <a href="mailto:&#110;&#105;&#99;&#111;&#108;&#97;&#115;&#46;&#97;&#117;&#100;&#101;&#98;&#101;&#114;&#116;&#64;&#99;&#110;&#97;&#109;&#46;&#102;&#114;">
		     <span>&#110;&#105;&#99;&#111;&#108;&#97;&#115;&#46;&#97;&#117;&#100;&#101;&#98;&#101;&#114;&#116;&#64;&#99;&#110;&#97;&#109;&#46;&#102;&#114;</span></a> -->
	     	     <a href="mailto:&#110;&#105;&#99;&#111;&#108;&#97;&#115;&#46;&#97;&#117;&#100;&#101;&#98;&#101;&#114;&#116;&#64;&#105;&#103;&#110;&#46;&#102;&#114;">
		     <span>&#110;&#105;&#99;&#111;&#108;&#97;&#115;&#46;&#97;&#117;&#100;&#101;&#98;&#101;&#114;&#116;&#64;&#105;&#103;&#110;&#46;&#102;&#114;</span></a>
     		</li>
		<!--
		<li>
			<svg class="icon" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 414.937 414.937" enable-background="new 0 0 414.937 414.937;"><g><path d="M159.138,256.452c37.217,36.944,80.295,72.236,97.207,55.195c24.215-24.392,39.12-45.614,92.854-2.761	c53.734,42.874,12.696,71.727-10.757,95.363c-27.064,27.269-128.432,1.911-228.909-97.804C9.062,206.71-17.07,105.54,10.014,78.258	c23.46-23.637,52.006-64.879,95.254-11.458c43.269,53.394,22.161,68.462-2.054,92.861	C86.31,176.695,121.915,219.501,159.138,256.452z M213.104,80.203c0,0-11.227-1.754-19.088,6.113	c-8.092,8.092-8.445,22.032,0.082,30.552c5.039,5.039,12.145,6.113,12.145,6.113c13.852,2.598,34.728,6.997,56.944,29.206	c22.209,22.208,26.608,43.084,29.206,56.943c0,0,1.074,7.106,6.113,12.145c8.521,8.521,22.46,8.174,30.552,0.082	c7.861-7.86,6.113-19.087,6.113-19.087c-4.399-28.057-17.999-57.365-41.351-80.716C270.462,98.203,241.153,84.609,213.104,80.203z	 M318.415,96.958c40.719,40.719,58.079,86.932,52.428,124.379c0,0-1.972,11.859,5.773,19.604	c8.718,8.718,22.535,8.215,30.695,0.062c5.243-5.243,6.385-13.777,6.385-13.777c4.672-32.361-1.203-97.464-64.647-160.901	C285.605,2.887,220.509-2.988,188.147,1.677c0,0-8.527,1.136-13.777,6.385c-8.16,8.16-8.656,21.978,0.061,30.695	c7.746,7.746,19.604,5.773,19.604,5.773C231.484,38.879,277.696,56.24,318.415,96.958z"/></g></svg>
			Phone:
			<a href="tel:+&#051;&#051;&#049;&#052;&#048;&#050;&#055;&#050;&#050;&#057;&#055;">
			<span>+&#051;&#051; &#049; &#052;&#048; &#050;&#055; &#050;&#050; &#057;&#055;</span></a>
		</li>
		-->
	    </ul>

	    Snail mail:
	    <!--
	    <blockquote>
		    Laboratoire CEDRIC, case 2LAB20<br />
		    Conservatoire national des arts et m√©tiers<br />
		    2 rue Cont√©<br />
		    75003 Paris, France
	    </blockquote>-->
	    <blockquote>
		    IGN - Laboratoire LASTIG, √©quipe STRUDEL<br />
		    73 avenue de Paris<br />
		    94160 Saint-Mand√©, France
	    </blockquote>

	    <!-- If you want to pay me a visit, I am at the <a href="https://www.cnam-paris.fr/m-inscrire/plan-du-cnam-paris-842440.kjsp">office numbered 37.1.41</a> at Cnam Paris (2 rue Cont√©, Paris, France). -->
        </section>
        
        <section class="news">
	    <h3>üìå Job offers</h3>
	    <!-- We have an open position as <a href="http://cedric.cnam.fr/lab/wp-content/uploads/2023/01/CnamParisPUIA2023.pdf">full professor of Artificial Intelligence</a> in the Vertigo team at Cnam! -->
	    No open position currently.

            <h2>üì∞ News</h2>
	    <p><strong>April 8th, 2024:</strong> It is with great pleasure that I can announce being a co-author on two accepted papers at the <a href="https://www.grss-ieee.org/events/earthvision-2024/">EarthVision 2024</a> CVPR worskhop:
	    	<ul>
			<li><em>Detecting Out-Of-Distribution Earth Observation Images with Diffusion Models</em>, a work led by Georges Le Bellier,</li>
			<li><em>Cross-sensor super-resolution of irregularly sampled Sentinel-2 time series</em>, a joint work led by Aimi Okabayashi, in collaboration with Simon Donike and Charlotte Pelletier.</li>
		</ul>

		Congratulations to Georges and Aimi for their respective first publication!
	    </p>
	    <p><strong>March 25th, 2024:</strong> Perla Doubinsky has masterfully defended her PhD thesis on controllable generative models for image editing and data augmentation. Bravo to the new doctor! üéâ</p>
	    <p><strong>March 20th, 2024:</strong> <a href="https://elias-ramzi.github.io/">Elias Ramzi</a> has brilliantly defended his PhD thesis on robust image retrieval with deep learning. Congrats doctor! üéâ</p>
	    <p><strong>December 1st, 2023:</strong> I am starting a new position as junior research director at IGN in Paris. I will be leading a new project on coupling heterogeneous ML-based and physics-based generative models for geodata analysis with a support from ANR.</p>
	    <p><strong>October 24th, 2023:</strong> Our <a href="https://hal.science/hal-04259058">paper</a> <em>Semantic Generative Augmentations for Few-Shot Counting</em> has been accepted to <a href="https://wacv2024.thecvf.com/">WACV 2024</a>. Congratulations Perla!</a>
	    <p><strong>October 2nd, 2023:</strong> L√©o G√©r√© (MSc. INSA Toulouse) has joined the Vertigo team as a PhD student on generative models for music sheet to performance translation and vice-versa. He will be advised by Florent Jacquemard (INRIA/Cnam), Philippe Rigaux (Cnam) and myself. Welcome L√©o!</p>
	    <p><strong>September 15th, 2023:</strong> <a href="https://arxiv.org/abs/2309.08250">New preprint</a> on the optimization of rank losses for image retrieval, led by Elias Ramzi. This is an extension of our previous work on (hierarchical) image retrieval. It introduces a new hierarchical version of the Google Landmarks v2 dataset. <a href="https://github.com/cvdfoundation/google-landmark">Check it out on GitHub!</a></p>
	    <p><strong>July 16th, 2023:</strong> I am at IGARSS 2023 in Pasadena until the end of the week. Ping me if you want to chat!</p>
	    <p><strong>June 9th, 2023:</strong> Our paper on <a href="https://arxiv.org/abs/2304.10508">semantic editing of images in the latent space of GANs using optimal transport</a> has been accepted for an oral presentation at <a href="https://cbmi2023.org">CBMI'23</a>! Congrats Perla!</p>
	    <p class="strike"><strong>April 18th, 2023:</strong> We are looking for PhD candidate on <a href="files/2023_Th√®se_SMI_level_generation.pdf">deep generative models (large image and language models) for video game level generation</a>, with Guillaume Levieux and myself. Apply before May 19th!</p>
	    <p><strong>March 22nd, 2023:</strong> <a href="https://hal.science/hal-04036414">New preprint</a> from Perla Doubinsky et al. on image editing by manipulating the GAN latent space using optimal transport!</p>
	    <p><strong>March 1st, 2023:</strong> Valerio Marsocci joines the Vertigo team as a postdoctoral fellow. He will work on self-supervised learning for Earth Observation imagery. Welcome Valerio!</p>
	    <p><strong>February 13, 2023:</strong> Armand Verstraete joins the Vertigo team as research engineer on the <a href="https://mage.science">MAGE</a> project, working on procedural generation of virtual cities. Welcome Armand!</a>
	    <p><strong>January 16, 2023:</strong> Georges Le Bellier (MSc. from Centrale Lille) has joined the Vertigo team as a PhD student on domain adaptation and self-supervised learning for Earth Observation, as a part of the <a href="https://mage.science">ANR MAGE</a> project. He will be advised by Nicolas Thome (Sorbonne), Marin Ferecatu (Cnam) and myself. Welcome Georges!</p>
	    <p><strong>November 30, 2022:</strong> We are hiring two interns on <a href="files/2022_stage_cog.pdf">hierarchical learning for better generalization</a> and <a class="strike" href="files/2022_stage_superresolution.pdf">super-resolution for sequences of satellite images</a>.</p>
	    <p><strong>October 23, 2022:</strong> Elias Ramzi will be at ECCV'2022 to present HAPPIER, our <a href="https://github.com/elias-ramzi/HAPPIER">differentiable criterion for hierarchical ranking</a>.</p>
	    <p><strong>August 22, 2022:</strong> Our PhD student Perla Doubinsky's paper on disentangled controls for GAN has been accepted for publication into <a href="https://www.sciencedirect.com/science/article/pii/S0167865522002501">Pattern Recognition Letters</a>. Congrats Perla!</p>
	    <p><strong>July 11, 2022:</strong> I am principal investigator on the <a href="https://anr.fr/fileadmin/aap/2022/selection/aapg-selection-2022-2-01072022-v2.pdf">newly accepted</a> MAGE project (2022-2026): <em>Mapping Aerial imagery with Game Engine data</em>. Job offers coming soon.</p>
	    <p><strong>July 6, 2022:</strong> I'll be at CAp-RFIAP 2022 in Vannes from July 6 to July 8 for the conference. I'll present some of our work on <a href="https://hal.archives-ouvertes.fr/hal-03678311v1">classifying apes vocalization</a> (french) and <a href="https://hal.archives-ouvertes.fr/hal-03678280v1">learning diversified behaviours by reinforcement for video games</a>.</p>
	    <p><strong>July 5, 2022:</strong> <a href="https://hal.archives-ouvertes.fr/hal-03712933">Our paper HAPPIER</a>, an extension of average precision for the hierarchical image retrieval setting, as been accepted at <a href="https://eccv2022.ecva.net/">ECCV2022</a>! Congrats Elias!</p>
	    <p><strong>June 1, 2022:</strong> Maxime Merizette (MSc. from ESGT) has joined the team as CIFRE PhD student working on semantic segmentation of indoor 3D point clouds. He will be advised by J√©r√¥me Verdun (Cnam/ESGT) and myself, in a collaboration with <a href="https://www.quarta.fr/">Quarta</a>. Welcome Maxime!</p>
	    <p><strong>May 31, 2022:</strong> I am co-organizing with Herv√© Le Borgne (CEA LIST) and Alexandre Beno√Æt (UGA) a GdR ISIS meeting on the control of generative models. See <a href="https://www.gdr-isis.fr/index.php/reunion/476/">the website</a> for the full program.</p>
	    <p><strong>April 4, 2022:</strong> I have received an unrestricted gift from the <a href="https://research.google/outreach/research-scholar-program/recipients/">Google Research Scholar Program</a>.</p>
	    <p><strong>February 2, 2022:</strong> <a href="https://hal.archives-ouvertes.fr/hal-03551457">Our work</a> on neural networks for efficient autoprecoding in MU-MIMO systems will be published at WCNC 2022. Congrats Xinying Cheng!
	    <p><strong>January 27, 2022:</strong> <a href="https://arxiv.org/abs/2111.00909">New preprint</a> on finding disentangled directions in GAN latent spaces with simple attribute balancing, from our PhD student Perla Doubinsky.
	    <p><strong>December 2021:</strong> Code for <em>ROADMAP</em>, our robust and decomposable average precision surrogate for image retrieval published in Neurips is now available on <a href="https://github.com/elias-ramzi/ROADMAP">GitHub</a> (thanks to Elias Ramzi!).</p>
	    <p><strong>November 2021:</strong> I was distinguished as an outstanding reviewer for BMVC 2021.</p>
	    <!--<p><strong>October 2021:</strong> <strong>INTERSHIP OFFER</strong> on digit and parcel detection in old maps using domain adaptation and transfer learning. See the <a href="static/2022-Stage_CEDRIC_GeF.pdf">full proposal (in French)</a>.</p>-->
	    <p><strong>October 2021:</strong> Our paper on <a href="https://hal.archives-ouvertes.fr/hal-03359605v1">robust and decomposable differentiable ranking for image retrieval</a> (ROADMAP) has been accepted into <a href="https://neurips.cc/">NeurIPS 2021</a>.</p>
	    <p><strong>September 2021:</strong> I was distinguished as one of the <a href="http://iccv2021.thecvf.com/outstanding-reviewers">outstanding reviewers of ICCV 2021</a>.</p>
	    <p><strong>July 2021:</strong> <a href="https://sites.google.com/site/charpelletier/">Charlotte Pelletier</a> and I received a 7k‚Ç¨ grant for a project on super-resolution of Sentinel-2 time series from <a href="http://www.gdr-isis.fr">GDR ISIS</a>.</p>
	    <p><strong>July 2021:</strong> 2 papers accepted just before the holidays:
	    <ul>
		    <li> 1 paper to ISMIR21: <a href="https://arxiv.org/abs/2107.14009">PKSpell</a>, our deep recurrent network for automated pitch spelling and key signature estimation from MIDI musical performances.</li>
		    <li> 1 paper to ECML/PKDD Graph Embedding and Mining Workshop: <a href="https://arxiv.org/abs/2108.11629">Web Image Context Extraction</a> using graph neural networks on the HTML DOM tree.</li>
	    </ul>
	    </p>
	    <p><strong>January 2021:</strong> Elias Ramzi (MSc. from CentraleSup√©lec) has joined the team for his PhD thesis on deep learning-based image retrieval for large logo databases. He will be advised by Nicolas Thome (Cnam), Cl√©ment Rambour (Cnam) and myself. Welcome Elias!</p>
	    <p><strong>December 2020:</strong> <span class="strike">The RL & Games project at CEDRIC laboratory is looking for a MSC. level intern for 5/6 months starting spring 2021 on producing diverse behaviours for AI using reinforcement learning in video games. See the <a href="files/2021_stage_rl_jeu.pdf">internship offer</a> (in French) for more details.</span> (this position has been filled)</p>
	    <p><strong>November 2020:</strong> Perla Doubinsky (MSc. from UTC) has joined the Vertigo team for her PhD thesis on the control of generative models. She will be advised by Michel Crucianu (Cnam), Herv√© Le Borgne (CEA LIST) and myself.  Welcome Perla!</p>
	    <p><strong>November 2020:</strong> <span class="strike">The Vertigo team is looking for a MSc. level intern for 5/6 months starting spring 2021 on equivariant neural networks for image classification and semantic segmentation. See the <a href="files/2021_stage_equivariance.pdf">internship offer</a> (in French) for more information</span>. (this position has been filled)</p>
	    <p><strong>October 2020:</strong> I was invited to organize a hands-on tutorial on machine learning for astrophysics at the  <a href="https://sftools-bigdata.sciencesconf.org/">SFtools-bigdata</a> conference. Code is available <a href="https://github.com/nshaud/ml_for_astro/blob/main/Hands_on_ML_for_astrophysics.ipynb">on my Github page</a>.</p>
	    <p><Strong>October 2020:</strong> A new specialization certificate on Artificial Intelligence has opened at Cnam. This program is tailored for professionnals that want to deepen their understanding of statistical learning, artificial intelligence and deep learning. Classes are tought either remotely or in the evening. Check out the curriculum <a href="https://formation.cnam.fr/rechercher-par-discipline/certificat-de-specialisation-intelligence-artificielle-1176377.kjsp">here (in French)</a>.</p>
	    <p><strong>June 2020:</strong> <span class="strike">I am looking for a PhD candidate for a fully-funded thesis on controlling generative networks for image synthesis in collaboration with CEA. Check out the <a href="http://cedric.cnam.fr/~crucianm/src/these_generative.pdf">full offer</a> fore more details.</span> This position has been filled.</p>
	    <p><strong>March 2020:</strong> We released <a href="https://ieee-dataport.org/open-access/sen12-flood-sar-and-multispectral-dataset-flood-detection">SEN12-FLOOD</a>, a flood detection dataset in Sentinel 1 and 2 time series.
	    <p><strong>January 2020:</strong> <span class=strike>Our laboratory is hiring a <a href="https://cedric.cnam.fr/lab/poste-mcf-ia-2020/">junior assistant professor</a> in Computer Science with a strong focus on machine learning and artificial intelligence. Application is done through the GALAXIE portal. It is a teaching and research permanent position in Paris.</span> Position has been filled.</p>
	    <p><strong>November 2019:</strong> We are looking to hire M2 interns from 5/6 months starting in spring 2020 on topics related to deep learning for image understanding. <span class="strike">I personally offer one internship on weakly-supervised semantic segmentation. Check out <a href="/static/2019_internship.pdf">the internship details</a></span>.
	    <span class="strike">I am also involved in another M2 internship offer on deep learning for MIMO radiocommunication (see the <a href="/static/2020_mimo_internship.pdf">subject</a> in French)</span> (these positions have been filled).
	    <span class="strike">Finally, I will co-advise with <a href="https://qwant.com">Qwant Research</a> a M2 internship on deep learning for fast webpage information extraction.</span>
	    If this interests you and feel free to contact me for more information.</p>
                <p><strong>September 2019:</strong> I am joining the <em>Conservatoire national des arts & m√©tiers</em> (Cnam) as an assistant professor in the Vertigo team.</p>
                <p><strong>August 2019:</strong> Our journal article on using signed distance transform regression to regularize semantic segmentation deep networks has been accepted for publication in CVIU.</p>
                <p><strong>July 2019:</strong> I will be at <a href="https://www.irit.fr/pfia2019/apia/">APIA 2019</a> in Toulouse from July 1st to July 5th to present our work on multi-modal text/image classification with deep nets. Feel free to come for a chat!</p>
                <p><strong>May 2019:</strong> Our paper on multi-modal text/image deep networks for document image classification has been accepted to <a href="https://www.irit.fr/pfia2019/apia/">APIA 2019</a> in Toulouse.</p>
                <p><strong>April 2019:</strong> I will be presenting at the <a href="http://www.gdr-isis.fr/index.php?page=reunion&idreunion=386">GdR ISIS meeting</a> on weakly and semi-supervised learning for image and video classification. My talk will detail some of the work I did at Quicksign on image/text clustering for document recognition.</p>
                <p><strong>April 2019:</strong> Our review on deep convolutional and recurrent neural networks for hyperspectral image classification has ben accepted for the IEEE Geoscience and Remote Sensing special issue on hyperspectral data. Preprint <a href="https://hal.archives-ouvertes.fr/hal-02104998">here</a>.</p>
                <p><strong>January 2019:</strong> I joined Quicksign R&D team as a research scientist.</p>
                <p><strong>October 2018:</strong> I successfully defended my PhD thesis! The manuscript (in french) is available <a href="/files/Manuscrit.pdf">here</a> with <a href="/files/planches_soutenance.pdf">slides</a>.</p>
                <p><strong>July 2018:</strong> I was at IGARSS'18 in Valencia, where I presented our work on <a href="https://arxiv.org/abs/1806.02583">generative adversarial network for hyperspectral samples synthesis</a>. You can find the code <a href="http://github.com/nshaud/HyperGANs">here</a>!</p>
                <p><strong>March 2018:</strong> We have one paper accepted for <a href="https://igarss2018.org/">IGARSS 2018</a> on generative adversarial networks for hyperspectral data synthesis. We'll also appear on the <a href="https://project.inria.fr/aerialimagelabeling/">Inria Aerial Image Labeling</a> benchmark write-up on building extraction.</p>
                <p><strong>January 2018:</strong> I ported the code of our deep network for aerial/satellite semantic segmentation to PyTorch for an easier use: <a href="https://github.com/nshaud/DeepNetsForEO">fork it on GitHub</a>!</p>
                <p><strong>November 2017:</strong> Our latest journal paper on data fusion for remote sensing data using deep fully convolutional networks is <a href="https://hal.archives-ouvertes.fr/hal-01636145">out</a> !</p>
                <p><strong>July 2017:</strong> I was at CVPR 2017 for the Earthvision workshop, where I presented <a href="https://nicolas.audebert.at/blog/pdfs/AudebertLeSauxLefevre_Earthvision_joint_poster.pdf">our work</a> on semantic mapping using deep nets and OpenStreetMap data.</p>
                <p><strong>June 2017:</strong> I collaborated with the <a href="http://www.polytech.univ-smb.fr/index.php?id=listic-accueil&L=1">LISTIC team</a> on using deep nets to perform semantic segmentation on Sentinel-2 images. This work will be presented at <a href="http://www.igarss2017.org/Papers/viewpapers.asp?papernum=1919">IGARSS'17</a> in Forth Worth, Texas.</p>
                <p><strong>June 2017:</strong> I presented at ORASIS 2017 our work on data fusion with deep networks for remote sensing (<a href="https://nicolas.audebert.at/blog/pdfs/AudebertLeSauxLefevre_ORASIS_fusion.pdf">slides</a>).</p>
                <p><strong>May 2017:</strong> Our submission on <a href="https://nicolas.audebert.at/blog/pdfs/AudebertLeSauxLefevre_CVPRW17_JointLearning.pdf">joint deep learning using optical and OSM data</a> for semantic mapping of aerial/satellite images has been accepted to the EarthVision 2017 CVPR Workshop !</p>
                <p><strong>April 2017:</strong> Our Remote Sensing journal paper on <a href="http://www.mdpi.com/2072-4292/9/4/368">vehicle segmentation for detection and classification</a> is out in open access on the MPDI website.</p>
                <p><strong>March 2017:</strong> My colleague <a href="https://sites.google.com/view/boulch">Alexandre Boulch</a> will present the SnapNet architecture for semantic segmentation of unstructured point clouds at Eurographics 3DOR workshop. It is the current state-of-the-art on the <a href="http://semantic3d.net">Semantic3D</a> dataset (<a href="https://github.com/aboulch/snapnet">code</a>).</p>
                <p><strong>March 2017:</strong> Our paper on <a href="https://nicolas.audebert.at/blog/pdfs/audebert_lesaux_lefevre_data_fusion_for_urban_remote_sensing.pdf">data fusion for remote sensing using deep nets</a> won the 2nd best student paper award at JURSE 2017 ! <a href="blog/pdfs/AudebertLeSauxLefevre_DataFusion-Slides.pdf">Slides</a> and <a href="blog/pdfs/AudebertLeSauxLefevre_DataFusion-Poster.pdf">poster</a> are available.</p>
                <p><strong>Februrary 2017:</strong> The code of the deep network we used for the ISPRS Vaihingen 2D Semantic Labeling Challenge is out on <a href="https://github.com/nshaud/DeepNetsForEO">Github</a> !<p>
                <p><strong>January 2017:</strong> We will present two invited papers at <a href="http://www.jurse2017.com/">JURSE 2017</a> !</p>
                <p><strong>November 2016:</strong> I will be at <a href="http://www.accv2016.org">ACCV'16</a> in Taipei to present our poster on semantic segmentation of Earth Observation using multi-scale and multimodal deep networks.</p>
                <p><strong>October 2016:</strong> I will be at <a href="https://2016.pycon.fr/">PyCon-fr</a> (the French Python conference) to speak about deep learning using Python (<a href="https://nicolas.audebert.at/blog/pdfs/pyconfr2016-deep-learning-naudebert.pdf">slides (in French)</a> and <a href="https://www.pycon.fr/2016/videos/deep-learning-votre-propre-cerveau-artificiel-avec-python.html">video (in French, too)</a>).</p> 
                <p><strong>September 2016:</strong> Our paper on the use of deep networks for object-based image analysis of vehicles in the ISPRS dataset has been distinguished by the "<em>Best Benchmarking Contribution Award</em>" at <a href="https://www.geobia2016.com/">GEOBIA 2016</a> !</p>
                <p><strong>September 2016:</strong> I will be at <a href="https://www.geobia2016.com/">GEOBIA 2016</a> in Enschede to talk about our work on object-based analysis of cars in remote sensing images using deep learning.</p>
                <p><strong>September 2016:</strong> Our paper on semantic segmentation for Earth Observation was accepted at <a href="http://accv2016.org">ACCV'16</a> for a poster presentation. Check out the state-of-the-art results on the <a href="http://www2.isprs.org/vaihingen-2d-semantic-labeling-contest.html">ISPRS Vaihingen 2D Semantic Labeling Challenge</a> !</p>
                <p><strong>July 2016:</strong> I will be at <a href="http://igarss2016.org/">IGARSS'16</a> in Beijing to present our work on superpixel-based semantic segmentation of aerial images.</p>
                <p><strong>April 2016:</strong> Our paper on region-based classification of remote sensing images using deep features has been accepted at <a href="http://igarss2016.org/">IGARSS'16</a> for an oral presentation.</p>
                <p><strong>October 2015:</strong> I started as a PhD student at ONERA and IRISA.</p>
        </section>

	<section class="projects">
	    <h2>ü•º Research projects</h2>

	    <h3>üßô MAGE (2022-2026)</h3>
	    <p><em><a href="https://mage.science">Website</a></em></p>
	    <p><strong>Participants:</strong> Nicolas Audebert (PI), Georges Le Bellier, Armand Verstraete, Valerio Marsocci, Guillaume Levieux, Charlotte Pelletier, Nicolas Thome, Devis Tuia.</p>
	    <p><strong>Funding:</strong> Agence Nationale de la Recherche</p>
	    <p><em>Mapping Aerial imagery with Game Engine data</em> (MAGE) aims to leverage procedural generation and modern rendering engines to generate labeled synthetic data for deep Earth Observation models. It investigates the following questions: how can we generate synthetic data of cities, before and after a natural disaster? How can make the simulated images look more realistic? How to train deep models on a mix of real unlabeled data and simulated labeled images? It is a four-year project funded as a <em>Young Researcher Grant</em> from ANR.</p>

	    <h3>üõ∞ SESURE (2021-2023)</h3>
	    <p><strong>Participants:</strong> Nicolas Audebert, Charlotte Pelletier, Simon Donike, Aimi Okabayashi.</p>
	    <p><strong>Funding:</strong> <a href="https://www.gdr-isis.fr/index.php/projets-de-recherche-isis/">GdR ISIS</a>.</p>
	    <p><em>SEntinel-2 SUper REsolution</em> (SESURE) aims to combine the high revisit frequency of the Sentinel-2 constellation with the very high spatial resolution of SPOT. To do so, we aim to train deep generative models to perform super-resolution by leveraging the temporal information contained in Sentinel-2 time series.</p>

	    <h3>üéÆ RL-Games (2020-2022)</h3>
	    <p><strong>Participants:</strong> Nicolas Audebert (PI), Guillaume Levieux, Cl√©ment Rambour, Rapha√´l Boige, Zineb Lahrichi.</p>
	    <p><strong>Funding:</strong> <a href="https://cedric.cnam.fr/lab/actions/">C√©dric laboratory</a>.</p>
	    <p>RL-Games investigates innovative applications of machine learning and reinforcement learning for video games and entertainment. In the first year, we investigated the limitations of training agents by reinforcement in game settings. We observed that, contrarily to players' expectations, RL agents exhibit behaviours with low diversity, and therefore low "fun". We studied the limitations of skill discovery algorithms as a way to learn diversity and introduced the concept of observer-based diversity in a RL setting.</p>

	</section>
        
        <section class="research">
            <h2>üë®‚Äçüî¨ Research areas</h2>
	    My research interests shifts with time but my work falls broadly into one of these three areas which overlap - more often than not.

            <h3>Representation learning</h3>

            Machine learning consists in training a computer to perform a task without explicitly detailing how to do so, but instead by creating models that leverage patterns in the data and perform inference from them.
            Especially interesting is <em>deep learning</em>, i.e. producing abstract representation of raw data that allows the computer to manipulate high-level concepts in numerical terms.
	    I am mostly interested in how machine learning can improve machine perception (image, sound and text processing and understanding) through useful <em>representations</em>.

            <h3>Earth Observation</h3>

            Remote sensing of Earth Observation data is a very broad field that aims to gather information about our planet and the objects that cover its surface.
            Earth Observation leverages satellite images (in the broad sense that radar, multispectral and infrared data are also images) to understand ecological phenomenon, map urban expansions or monitor natural disasters.
            My work focuses on processing and making sense of the large volumes of data acquired by the sensors for land cover mapping, change detection and image interpreation through automated means.

            <!--<h3>Data fusion</h3>

            Information processing in the modern age constrain computers to work with a large panel of data types: images, videos, texts, sounds, Lidar, radar, sonar and other various kinds of sensors.
            In the real world, humans are able to sense the world through multiple modalities by analog means.
	    Data fusion aims to reproduce these abilities in the digital world for cross-modal data mining and pattern recognition, e.g. making sense of complex manuscripts or understanding movie clips.-->

	    <h3>Machine learning and games</h3>

	    I have recently picked up machine learning for games as an interest. I have led a small project at Cnam on improving reinforcement learning to generate diverse and challenging AI in video games (collab. with Cl√©ment Rambour and Guillaume Levieux).
	    My goal is to better understand what makes an interesting AI or environment from a gameplay point of view. My focus is on how to use machine learning in clever ways to improve the gaming experience.

            <h2>üë• Collaborators</h2>

	    I am sometimes looking for collaborators (either at masters or PhD level). You can cold email me but check that my current research is interesting to you and is something that you feel comfortable working with.

	    <h3>PhD students</h3>

	    I currently advise three PhD students:
	    <ul>
		    <li><a href="https://cedric.cnam.fr/lab/author/gerel/">L√©o G√©r√© (Cnam)</a>: üéº Grammar-based generative models for music transcription and generation, with Florent Jacquemard (Cnam/Inria) and Philippe Rigaux (Cnam), since October 2023.</li>
		    <li><a href="https://gle-bellier.github.io/">Georges Le Bellier</a> (Cnam): üåê domain adaptation and self-supervised learning for Earth Observation, with Marin Ferecatu (Cnam), since January 2023.</li>
		    <li>Maxime Merizette (Cnam/ESGT/Quarta): üè† semantic segmentation of extremely detailed 3D point clouds in indoor settings, with J√©r√¥me Verdun (Cnam/ESGT) and Pierre Kervella (Quarta), since June 2022.</li>
	    </ul>

	    <h4>Past</h4>

	    <ul>
		    <li><a href="https://cedric.cnam.fr/lab/author/pdoubinsky/">Perla Doubinsky</a> (Cnam/CEA): üé¨ controlling generative models to steer image generation, with Michel Crucianu (Cnam) and Herv√© Le Borgne (CEA). November 2020 - March 2024.</li>
		    <li><a href="https://elias-ramzi.github.io/">Elias Ramzi</a> (Cnam/Coexya): üîé robust image retrieval with deep learning, with Nicolas Thome (Cnam), Cl√©ment Rambour (Cnam) and Xavier Bitot (Coexya). January 2021 - March 2024.</li>
	    </ul>


	    <h3>Postdocs</h3>

	    <p>Since april 2020, I work with <a href="https://marionlaporte.wordpress.com/">Marion Laporte</a> from the Origins of Speech team at ICSD on building a vocal space of apes. We aim to leverage statistical models to characterize the graded nature of chimpanzee and bonobo vocalizations.</p>

	    <h4>Past</h4>
	    <ul>
	    	  <li>March 2023 to march 2024: I worked with <a href="https://sites.google.com/uniroma1.it/valeriomarsocci">Valerio Marsocci</a> at Cnam on self-supervised learning approaches to Earth Observation. We built large pretrained models that can be applied to multiple downstream tasks across sensors.</li>
		  <li>September 2020 to september 2022: I have been working with <a href="https://ipag.osug.fr/~robitaij/index.html">Jean-Fran√ßois Robitaille</a> from IPAG on deep learning multispectral image processing to detect multiscale interstellar structures. Our goal was to use data mining to detect interstellar objects in large clouds.</li>
		  <li>September 2019 to september 2020: I worked with <a href="https://cedric.cnam.fr/lab/author/crambour">Cl√©ment Rambour</a> (Cnam/ONERA) during his post-doc on multi-temporal analysis of SAR and optical data. We focused our work on flood detection in SAR/multispectral time series.</li>
	    </ul>
		
            <h3>MSc. students</h3>

	    <h4>Current</h4>
	    <!--I am not supervising any master student currently.-->

	    I am currently advising one student for their master internship:
	    <ul>
		    <li>Charles Vin (Cnam): self-supervising learning on satellite image time series with ranking losses, MSc. from Sorbonne Universit√©. Internship on representation learning for Earth Observation data, by learning how to order sequences of Sentinel-2 images (with C. Rambour).</li>
	    </ul>

	    <h4>Past</h4>

	    I have supervised or co-supervised several students for their research internship:

	    <h5>2018</h5>
            <ul>
		 <li><a href="https://javicastillo.ml/">Javiera Navarro-Castillo</a> (ONERA): Towards the "ImageNet" of remote sensing, MSc. from √âcole Polytechnique. Internship on large-scale semi-supervised semantic segmentation (with A. Boulch, B. Le Saux and S. Lef√®vre). Now a post-doc at EPFL.</li>
	    </ul>
	    <h5>2019</h5>
	    <ul>
                <li><a href="https://fr.linkedin.com/in/adel-redjimi-05183713a">Adel Redjimi</a> (Quicksign): Semi-supervised learning for document image classification, MSc. from INP Grenoble. Internship on document classification with scarce supervision and unlabeled data (with K. Slimani).</li>
	    </ul>
	    <h5>2020</h5>
	    <ul>
		<li><a href="https://fr.linkedin.com/in/chendang">Chen Dang</a> (Qwant): fast webpage information extraction. MSc. from Universit√© Paris Sciences & Lettres. Internship on webpage image context extraction for indexing (with R. Fournier-S'niehotta and H. Randrianarivo). Now a PhD student at Orange Labs.</li>
		<li><a href="https://tn.linkedin.com/in/samar-chebbi">Samar Chebbi</a> (Cnam): deep learning for linear precoding in massive MIMO systems with power amplifiers, Sup'com engineering diploma (with R. Zayani and M. Ferecatu).</li>
		<li><a href="https://fr.linkedin.com/in/yannis-karmim-59257b18b">Yannis Karmim</a> (Cnam): semi-supervised learning for semantic segmentation, MSc. from Sorbonne Universit√©. Internship on equivariance constraints for segmentation models (with N. Thome). Now a PhD student at Cnam.</li>
	    </ul>
	    <h5>2021</h5>
	    <ul>
		<li><a href="https://egiob.github.io/markdown-cv/">Rapha√´l Boige</a> (Cnam): Internship on learning agents with diversified behaviours for video games by reinforcement (with G. Levieux and C. Rambour). MSc. Data Science from T√©l√©com Paris/√âcole Polytechnique. Now a research scientist at InstaDeep.</li>
		<li><a href="https://fr.linkedin.com/in/jo%C3%A3o-pedro-ara%C3%BAjo-ferreira-campos-bb14931a9">Jo√£o Pedro Ara√∫jo Ferreira Campos</a> (Cnam): Internship on spatially-equivariant representation learning for remote sensing and medical images (with C. Rambour and N. Thome). MSc. ENSTA Paris. Now a PhD candidate at Universidade Federal de Minas Gerais.</li>
	    </ul>
	    <h5>2022</h5>
	    <ul>
		<li><a href="https://www.donike.net/">Simon Donike</a> (Cnam): super-resolution of Sentinel-2 time series, with Charlotte Pelletier and Dirk Tiede. MSc. in geodata science from U. Salzsburg/U. South Brittany. Now a PhD student a University of Valencia.</li>
		<li><a href="https://www.linkedin.com/in/tiecoumba-ibrahim-tamela">Ibrahim Tamela</a> (Cnam): digit detection in historical cadastre rasters, with Jean-Michel Follin, √âlisabeth Simonetto and Fr√©d√©ric Durand (Cnam/ESGT). Specialized master in photogrammetry and geomatics from ENSG.</li>
		<li><a href="https://fr.linkedin.com/in/zineblahrichi">Zineb Lahrichi</a> (Ubisoft): Controlled procedural generation of video game levels, with Guillaume Levieux (Cnam) and Ludovic Denoyer (Ubisoft). MSc. from CentraleSup√©lec.</li>
            </ul>
	    <h5>2023</h5>
	    <ul>
		<li><a href="https://www.linkedin.com/in/aimi-okabayashi-107a941b8?originalSubdomain=fr">Aimi Okabayashi</a> (Cnam/IRISA): diffusion models for super-resolution of satellite image time series, with Charlotte Pelletier. MSc. from ENSTA.</li>
	    </ul>
        </section>

        <section class="code">
            <h2>üíª Code & datasets</h2>

	    <p>Most of my research code is released under permissive open source licenses on <a href="https://github.com/nshaud">GitHub</a>.</p>

	    <h3>MiniFrance</h3>
	    <p>During my PhD thesis, I collected and built the MiniFrance dataset which combines open source land cover geodata from Copernicus Urban Atlas and aerial images from IGN. It was then improved upon by Javiera Castillo and <a href="https://ieee-dataport.org/open-access/minifrance">published in open access</a>.</p>

	    <h3>QS-OCR</h3>
	    <p>During my time at Quicksign, we released <a href="https://github.com/Quicksign/ocrized-text-dataset">QS-OCR</a>, a text/image classification dataset using OCR'd document images.</p>

	    <h3>SEN12-FLOOD</h3>
	    <p>Improving on the 2019 MediaEval Multimedia Satellite Task, we designed and released in 2020 <a href="https://clmrmb.github.io/SEN12-FLOOD/">SEN12-FLOOD</a>, a multimodal SAR/multispectral dataset (Sentinel 1 and 2) for flood event detection in remote sensing time series.</p>
        </section>

        <section class="publications">
            <h2>üìö Publications</h2>

            <h3 id="journals">Journals:</h3>
            <ul>
		    <li>2023, <em><a href="https://arxiv.org/abs/2309.08250">Optimization of Rank Losses for Image Retrieval</a></em>, Elias Ramzi, <strong>Nicolas Audebert</strong>, Cl√©ment Rambour, Andr√© Araujo, Xavier Bitot, Nicolas Thome, preprint.</li>
		    <li>2022, <em><a href="https://cnam.hal.science/hal-03404279">Multi-attribute balanced sampling for disentangled GAN controls</a></em>, Perla Doubinsky, <strong>Nicolas Audebert</strong>, Michel Crucianu, Herv√© Le Borgne, Pattern Recognition Letters.</li>
		    <li>2021, <em><a href="https://hal.science/hal-03132924">Semi-Supervised Semantic Segmentation in Earth Observation: The MiniFrance Suite, Dataset Analysis and Multi-task Network Study</a></em>, Javiera Castillo-Navarro, Bertrand Le Saux, Alexandre Boulch, <strong>Nicolas Audebert</strong>, S√©bastien Lef√®vre, Machine Learning Journal.</li>
		    <li>2019, <em><a href="https://hal.science/hal-02104998">Deep learning for classification of hyperspectral data: a comparative review</a></em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, IEEE Geosciences and Remote Sensing Magazine.</li>
		    <li>2019, <em><a href="https://hal.science/hal-02277621">Distance transform regression for spatially-aware deep semantic segmentation</a></em>, <strong>Nicolas Audebert</strong>, Alexandre Boulch, Bertrand Le Saux, S√©bastien Lef√®vre, Computer Vision and Image Understanding.</li>
		    <li>2018, <em><a href="https://hal.science/hal-01636145">Beyond RGB: Very High Resolution Urban Remote Sensing With Multimodal Deep Networks</a></em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, ISPRS Journal of Photogrammetry and Remote Sensing, Elsevier, 2018.</li>
		    <li>2017, <em><a href="https://hal.science/hal-01529624">Segment-before-Detect: Vehicle Detection and Classification through Semantic Segmentation of Aerial Images</a></em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, Remote Sensing, MDPI, 2017.</li>
                </ul>
                <h3 id="conferences">Conferences:</h3>
                <ul>
			<li>2024, <em><a href="">Detecting Out-Of-Distribution Earth Observation Images with Diffusion Models</a></em>, Georges Le Bellier, <strong>Nicolas Audebert</strong>, EarthVision - CVPR workshop, Seattle.</li>
			<li>2024, <em><a href="">Cross-sensor super-resolution of irregularly sampled Sentinel-2 time series</a></em>, Aimi Okabayashi, <strong>Nicolas Audebert</strong>, Simon Donike, Charlotte Pelletier, EarthVision - CVPR workshop, Seattle.</li>
			<li>2024, <em><a href="https://hal.science/hal-04259058">Semantic Generative Augmentations for Few-Shot Counting</a></em>, Perla Doubinsky, <strong>Nicolas Audebert</strong>, Michel Crucianu, Herv√© Le Borgne, WACV 2024, Waikoloa.</li>
			<li>2023, <em><a href="https://hal.science/hal-04036414">Wasserstein loss for Semantic Editing in the Latent Space of GANs</a></em>, Perla Doubinsky, <strong>Nicolas Audebert</strong>, Michel Crucianu, Herv√© Le Borgne, CBMI 2023, Orl√©ans.</li>
			<li>2022, <em><a href="https://hal.science/hal-03712933">Hierarchical Average Precision Training for Pertinent Image Retrieval</a></em>, Elias Ramzi, <strong>Nicolas Audebert</strong>, Nicolas Thome, Cl√©ment Rambour, Xavier Bitot, ECCV 2022, Tel-Aviv.</li>
			<li>2022, <em><a href="https://arxiv.org/abs/2202.03190">Efficient Autoprecoder-based deep learning for massive MU-MIMO Downlink under PA Non-Linearities</a></em>, Xinying Cheng, Rafik Zayani, Marin Ferecatu, <strong>Nicolas Audebert</strong>, WCNC 2022, Austin.</li>
			<li>2021, <em><a href="https://hal.science/hal-03359605">Robust and Decomposable Average Precision for Image Retrieval</a></em>, Elias Ramzi, Nicolas Thome, Cl√©ment Rambour, <strong>Nicolas Audebert</strong>, Xavier Bitot, NeurIPS 2021, virtual.</li>
			<li>2021, <em><a href="https://hal.science/hal-03300102">PKSpell: Data-Driven Pitch Spelling and Key Signature Estimation</a></em>, Francesco Foscarin, <strong>Nicolas Audebert</strong>, Rapha√´l Fournier-S'Niehotta, ISMIR, virtual.</li>
			<li>2021, <em><a href="https://hal.science/hal-03324009">Web Image Context Extraction with Graph Neural Networks and Sentence Embeddings on the DOM tree</a></em>, Chen Dang, Hicham Randrianarivo, Rapha√´l Fournier-S'Niehotta, <strong>Nicolas Audebert</strong>, ECML/PKDD: GEM workshop, virtual.</li>
			<li>2020, <em><a href="https://hal.science/hal-02924091">A Real-World Hyperspectral Image Processing Workflow for Vegetation Stress and Hydrocarbon Indirect Detection</a></em>, Dominique Dubucq, <strong>Nicolas Audebert</strong>, V√©ronique Achard, Alexandre Alakian, Sophie Fabre, Anthony Credoz, Philippe Deliot, Bertrand Le Saux, XXIV ISPRS Congress, Nice.</li>
			<li>2020, <em><a href="https://hal.science/hal-02924220">Flood Detection in Time Series of Optical and SAR images</a></em>, Cl√©ment Rambour, <strong>Nicolas Audebert</strong>, Elise Koeniguer, Bertrand Le Saux, Michel Crucianu, Mihai Datcu, XXIV ISPRS Congress, Nice.</li>
			<li>2019, <em><a href="https://hal.science/hal-02163257">Multimodal deep networks for text and image-based document classification</a></em>, <strong>Nicolas Audebert</strong>, Catherine Herold, Kuider Slimani, C√©dric Vidal, ECML/PKDD Workshop on Multi-view Learning, W√ºrzburg.</li>
			<li>2018, <em><a href="https://hal.science/hal-01809872">Generative Adversarial Networks for Realistic Synthesis of Hyperspectral Samples</a></em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, IGARSS, Valencia, 2018.</li>
                <li>2017, <em><a href="https://hal.science/hal-01672870">Couplage de donn√©es g√©ographiques participatives et d‚Äôimages a√©riennes par apprentissage profond</a></em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, GRETSI, Juan-les-Pins, 2017.</li>
                <li>2017, <em><a href="https://hal.science/hal-01523573">Joint Learning from Earth Observation and OpenStreetMap Data to Get Faster Better Semantic Maps</a></em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, EarthVision - CVPR Workshop, Honolulu, 2017 (poster).</li>
                <li>2017, <em><a href="https://diglib.eg.org/bitstream/handle/10.2312/3dor20171047/017-024.pdf?sequence=1&isAllowed=y">Unstructured point cloud semantic labeling using deep segmentation networks</a></em>, Alexandre Boulch, Bertrand Le Saux, <strong>Nicolas Audebert</strong>, Eurographics 3DOR, Lyon, 2017.</li>
                <li>2017, <em><a href="https://hal.science/hal-01438499">Fusion of Heterogeneous Data in Convolutional Networks for Urban Semantic Labeling</a></em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, JURSE, Dubai, 2017 (slides, poster).</li>
                <li>2016, <em><a href="https://hal.science/hal-01360166">Semantic Segmentation of Earth Observation Data Using Multimodal and Multi-scale Deep Networks</a></em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, ACCV, Taipei, 2016 (poster).</li>
                <li>2016, <em><a href="https://hal.science/hal-01320010">On the usability of deep networks for object-based image analysis</a></em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, GEOBIA, Enschede, 2016 (slides).</li>
                <li>2016, <em><a href="https://hal.science/hal-01320016">How useful is region-based classification of remote sensing images in a deep learning framework ?</a></em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, IGARSS, Beijing, 2016 (slides).</li>
                <li>2016, <em>Structural classifiers for contextual semantic labeling of aerial images</em>. Hicham Randrianarivo, Bertrand Le Saux, <strong>Nicolas Audebert</strong>, Michel Crucianu, Marin Ferecatu, Big Data from Space (BiDS), Tenerife, 2016.</li>
                </ul>
                <h3 id="communications">Communications :</h3>
                <ul>
		<li>2023, <em>Fusion d‚Äôinformations pour la segmentation s√©mantique de nuages de points d‚Äôint√©rieurs de b√¢timents</em>, Maxime M√©rizette, <strong>Nicolas Audebert</strong>, J√©r√¥me Verdun, Pierre Kervella, ORASIS 2023 (Journ√©es des jeunes chercheurs en vision par ordinateur), Carquerrane, 2023.</li>
		<li>2022, <em><a href="https://hal.science/hal-03678280">Now you see me: finding the right observation space to learn diverse behaviours by reinforcement in games</a></em>, Rapha√´l Boige, <strong>Nicolas Audebert</strong>, Cl√©ment Rambour, Guillaume Levieux, Conf√©rence sur l'Apprentissage automatique (CAp), Vannes, 2022.</li>
		<li>2022, <em><a href="https://hal.science/hal-03678311">Caract√©risation du r√©pertoire vocal des chimpanz√©s par apprentissage profond</a></em>, <strong>Nicolas Audebert</strong>, Marion Laporte, Congr√®s Reconnaissance des Formes, Image, Apprentissage et Perception (RFIAP), Vannes, 2022.</li>
		<li>2022, <em><a href="">Contr√¥le de la cardinalit√© par navigation dans l'espace latent des GANs</a></em>, Perla Doubinsky, <strong>Nicolas Audebert</strong>, Michel Crucianu, Herv√© Le Borgne, Congr√®s Reconnaissance des Formes, Image, Apprentissage et Perception (RFIAP), Vannes, 2022.</li>
                <li>2019, <em><a href="https://hal.science/hal-02163257">Multimodal deep networks for text and image-based document classification</a></em>, <strong>Nicolas Audebert</strong>, Catherine Herold, Kuider Slimani, C√©dric Vidal, APIA (Applications Pratiques de l‚ÄôIntelligence Artificielle), Toulouse, 2019 (slides).</li>
		<li>2018, <em><a href="https://hal.science/hal-01809991">Segmentation s√©mantique profonde par r√©gression sur cartes de distances sign√©es</a></em>, <strong>Nicolas Audebert</strong>, Alexandre Boulch, Bertrand Le Saux, S√©bastien Lef√®vre, Congr√®s Reconnaissance des Formes, Image, Apprentissage et Perception (RFIAP), Marne-la-Vall√©e, 2018.</li>
		<li>2017, <em><a href="https://hal.science/hal-01672871">R√©seaux de neurones profonds et fusion de donn√©es pour la segmentation s√©mantique d‚Äôimages a√©riennes</a></em>, <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, ORASIS 2017 (Journ√©es des jeunes chercheurs en vision par ordinateur), Colleville-sur-Mer, 2017 (slides).</li>
                <li>2016, <em>Deep Learning for Remote Sensing</em>. <strong>Nicolas Audebert</strong>, Alexandre Boulch, Adrien Lagrange, Bertrand Le Saux, S√©bastien Lef√®vre, 16th ONERA-DLR Aerospace Symposium (ODAS), Oberpfaffenhofen, 2016.</li>
                <li>2016, <em>Deep learning for aerial cartography</em> (poster). <strong>Nicolas Audebert</strong>, Bertrand Le Saux, S√©bastien Lef√®vre, Statlearn Workshop, Vannes, 2016.</li>
            </ul>

        </section>

        <section class="teaching">
            <h2>üë®‚Äçüè´ Teaching</h2>

            <h3>Current classes</h3>
	    I teach some classes related to image processing and machine learning at <a href="https://ensg.eu/">ENSG</a>.

            <h3>Past courses</h3>
            At Cnam, I mostly taught pattern recognition, image processing and machine learning for text and image understanding:
	    <ul>
		    <li><a href="http://cedric.cnam.fr/vertigo/Cours/ml/">RCP208</a>: Pattern Recognition (practical classes).</li>
		    <li><a href="https://cedric.cnam.fr/vertigo/Cours/ml2/">RCP209</a>: Supervised Learning and Neural Networks (deep learning).</li>
		    <li><a href="https://cedric.cnam.fr/vertigo/Cours/RCP211/">RCP211</a>: Advanced Artificial Intelligence (generative models).</li>
		    <!--<li><a href="https://cedric.cnam.fr/vertigo/Cours/RCP217/">RCP217</a>: Artificial Intelligence for Multimedia Applications (deep learning for audio processing).</li>-->
		    <!--<li><a href="https://cedric.cnam.fr/vertigo/Cours/RCP216/">RCP216</a>: Large Scale Machine Learning with Spark.</li>-->
	    </ul>

	    I also taught the <a href="https://nshaud.github.io/stmn-analyse-images-2D/">2D image processing</a> module at Cnam/ENJMIN (engineer cycle).

            In the past I have taught <a href="http://imagine.enpc.fr/~monasse/Info/">"Introduction to C++ programming"</a> and <a href="http://imagine.enpc.fr/~monasse/Algo/">"Algorithms"</a> classes at √âcole Nationale des Ponts et Chauss√©es.
            Some resources are still available in the <a href="/legacy/teaching/ENPC/">archive</a>.
        </section>

        <section class="resume">
            <h2>üó£ Resume</h2>
            <h3>Short vitae</h3>

            <ul>
		<li>Since Dec. 2023: Junior Research Director at <em>Institut National de l'Information G√©ographique et Foresti√®re</em> (IGN).</li>
                <li>Since Sept. 2019: Associate Professor in Computer Science at <em>Conservatoire national des arts & m√©tiers</em> (Cnam).</li>
                <li>Jan. 2019 - Aug. 2019: Research scientist in deep learning and computer vision at <a href="https://quicksign.com">Quicksign</a>.</li>
                <li>Oct. 2015 - Oct. 2018: PhD in Computer Science at <a href="https://www.onera.fr"><abbr title="Office national d'√©tudes et de recherche en a√©rospatial">ONERA</abbr></a> and <a href="https://www.irisa.fr"><abbr title="Institut de recherche en informatique et syst√®mes al√©atoires">IRISA</abbr></a>.</li>
                <li>2015: MEng. in Computer Science from Sup√©lec.</li>
                <li>2015: MSc. in Human-Computer Interaction from Universit√© Paris-Sud.</li>
            </ul>

            <h3>Community service</h3>

            <ul>
		<li>Outstanding reviewer for ICCV 2021, BMVC 2021.</li>
                <li>Reviewer for ECCV, CVPR, ICCV, CVIU, BMVC, IEEE JSTARS, IEEE TGRS, IEEE TIP, ISPRS Journal of Photogrammetry and Remote Sensing, MDPI Remote Sensing‚Ä¶ See my reviewer record on <a href="https://publons.com/researcher/3038492/nicolas-audebert/peer-review/">Publons</a>.</li>
		<li>Program committee member for EarthVision in <a href="https://www.grss-ieee.org/earthvision2019/people.html">2019</a>, <a href="https://www.grss-ieee.org/earthvision2020/people.html">2020</a>, <a href="http://www.classic.grss-ieee.org/earthvision2021/">2021</a>, <a href="https://www.grss-ieee.org/events/earthvision-2022/">2022</a> and <a href="https://www.grss-ieee.org/events/earthvision-2023/">2023</a> (CVPR Workshop), MACLEAN <a href="https://mdl4eo.irstea.fr/maclean-machine-learning-for-earth-observation/">2019</a>, <a href="https://sites.google.com/view/maclean2019/">2020</a>, <a href="https://sites.google.com/view/maclean21/">2021</a>, <a href="https://sites.google.com/view/maclean22">2022</a> and <a href="https://sites.google.com/view/maclean23">2023</a> (ECML/PKDD Workshop).</li>
            </ul>
        </section>

        <section class="misc">
            <h2>ü§∑ Misc</h2>
                
	    <p>I am an occasional translator for the <a href="https://github.com/python/python-docs-fr">French Python documentation project</a>.</p>

	    <p>I have an <a href="https://en.wikipedia.org/wiki/Erd%C5%91s_number">Erd≈ës number</a> of <a href="https://www.csauthors.net/distance/paul-erdos/nicolas-audebert">4</a>. I don't have an <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Bacon_number">Erd≈ës‚ÄìBacon number</a> yet but you never know‚Ä¶
	    </p>

            My <a href="https://en.wikipedia.org/wiki/Kardashian_Index">Kardashian index</a> is 1.29.
        </section>
        </div>
        </div>

        <footer>
            Nicolas Audebert - <a href="https://creativecommons.org/licenses/by-nc/2.0/fr/deed.en">CC-BY-NC 2.0</a> - 2023
        </footer>

    </body>
</html>
